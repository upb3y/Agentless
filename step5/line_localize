# -*- coding: utf-8 -*-
"""Copy of javaSearchLocation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fsBZrFnInKN90NLRb3-90pVKrexMmXZk
"""

!pip install datasets



from datasets import load_dataset

ds = load_dataset("Daoguang/Multi-SWE-bench")

# Select the Java dataset
java_data = ds["java_verified"]

# Show dataset details
print(f"Dataset size: {len(java_data)} rows")
print(f"Columns: {java_data.column_names}")

# Show a sample entry
print("\nSample Entry:")
print(java_data[0])  # First row
print(java_data[10])
print(java_data[20])

from datasets import load_dataset
import pandas as pd

# Load dataset
ds = load_dataset("Daoguang/Multi-SWE-bench")

# Extract PRs where tests were failing but passed after the PR
successful_fixes = [entry for entry in ds["java_verified"] if entry["FAIL_TO_PASS"] != "[]"]

# Convert to DataFrame for better visualization
successful_fixes_df = pd.DataFrame(successful_fixes)

# Display the first few rows
print(successful_fixes_df.head())

pip install openai

import re
from datasets import load_dataset
from openai import OpenAI

# 用 OpenAI API 密钥实例化客户端，请替换为你的实际 API 密钥
client = OpenAI(api_key="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")  # 替换为你的实际 API Key


def extract_method_patch_snippet(patch_text, method_name):
    """
    Splits the diff patch into hunks and returns the first hunk that contains the method name.
    """
    parts = re.split(r"(@@.*?@@)", patch_text)
    # Iterate over hunk header and content pairs
    for i in range(0, len(parts) - 1, 2):
        hunk_header = parts[i]
        hunk_content = parts[i + 1]
        hunk = hunk_header + hunk_content
        if method_name in hunk:
            return hunk
    return None

def call_openai_for_fault_line_from_patch(patch_snippet, reason, num_samples=5):
    """
    使用 patch snippet 和 fault reason 构造 prompt，多次调用 OpenAI API（采样以降低方差），
    并返回所有候选行号的并集。
    """
    prompt = f"""Analyze the following Java diff patch snippet (with annotated line numbers):

{patch_snippet}

Reason for fault localization:
{reason}

Please identify and return ONLY the exact line number(s) (absolute line number in the new version of the file) where the error is most likely located.
Return only the integer(s) separated by spaces if there are multiple.
"""
    candidate_lists = []
    for _ in range(num_samples):
        try:
            response = client.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2  # 降低温度以获得更稳定的答案
            )
            output = response.choices[0].message.content.strip()
            candidate_lines = parse_llm_candidates(output)
            candidate_lists.append(candidate_lines)
        except Exception as e:
            print("OpenAI API error:", e)
    # 求所有采样返回的候选行号的并集并排序
    all_candidates = [num for sublist in candidate_lists for num in sublist]
    if not all_candidates:
        return []
    return sorted(set(all_candidates))


def find_faulty_line_in_patch(file_path, method_name, dataset):
    """
    Heuristically searches patch diffs in the dataset for the given file and method.
    Returns a mapping of PR IDs to candidate modified line numbers.
    """
    matching_entries = [
        entry for entry in dataset["java_verified"]
        if file_path in entry["patch"] and method_name in entry["patch"]
    ]

    faulty_lines_by_pr = {}
    for entry in matching_entries:
        patch_text = entry["patch"]
        faulty_lines = []
        current_line = None
        for line in patch_text.split("\n"):
            # Look for diff hunk header lines: @@ -old_line,old_length +new_line,new_length @@
            if line.startswith("@@"):
                m = re.search(r"\+(\d+),\d+", line)
                if m:
                    current_line = int(m.group(1))
            else:
                if current_line is not None:
                    # Consider only added lines (starting with '+', excluding diff metadata)
                    if line.startswith("+") and not line.startswith("+++"):
                        if method_name in line:
                            faulty_lines.append(current_line)
                        current_line += 1
        if faulty_lines:
            faulty_lines_by_pr[entry["pull_number"]] = faulty_lines
    return faulty_lines_by_pr

def localize_faulty_line_with_llm(input_data):
    """
    Main pipeline using only dataset patch diffs.
    For each input item (file, class, method, reason):
      1. Filters patches that reference the file and method.
      2. Extracts the diff hunk (patch snippet) related to the method.
      3. Optionally runs a heuristic to extract candidate line numbers.
      4. Queries Gemini with the patch snippet and reason to pinpoint the exact error line.

    Returns a mapping from (file_path, method_name) to the Gemini LLM output and candidate patch lines.
    """
    ds = load_dataset("Daoguang/Multi-SWE-bench")
    results = {}

    for item in input_data:
        file_path = item["file_path"]
        method_name = item["method_name"]
        reason = item.get("reason", "")

        print(f"Processing {file_path} - {method_name}")

        # Heuristic patch analysis
        patch_fault_lines = find_faulty_line_in_patch(file_path, method_name, ds)
        if patch_fault_lines:
            print(f"Heuristic patch analysis candidate lines: {patch_fault_lines}")
        else:
            print("No candidate lines found via patch analysis.")

        # Extract a patch snippet for LLM analysis from the first matching entry
        candidate_snippet = None
        for entry in ds["java_verified"]:
            if file_path in entry["patch"] and method_name in entry["patch"]:
                candidate_snippet = extract_method_patch_snippet(entry["patch"], method_name)
                if candidate_snippet:
                    break

        if candidate_snippet:
            llm_line = call_openai_for_fault_line_from_patch(candidate_snippet, reason)
        else:
            print("No patch snippet available for LLM analysis.")
            llm_line = None

        results[(file_path, method_name)] = {"llm_line": llm_line, "patch_lines": patch_fault_lines}

    return results

# --- Example usage ---

input_data = [{
    "file_path": "dubbo-common/src/main/java/org/apache/dubbo/common/utils/CompatibleTypeUtils.java",
    "class_name": "CompatibleTypeUtils",
    "method_name": "compatibleTypeConvert",
    "reason": "The method compatibleTypeConvert had issues handling LocalTime parsing correctly, which could lead to serialization errors."
}]

if __name__ == "__main__":
    results = localize_faulty_line_with_llm(input_data)
    for key, info in results.items():
        file_path, method_name = key
        print(f"\nFile: {file_path}\nMethod: {method_name}")
        if info["llm_line"] is not None:
            print(f"Gemini LLM identified faulty line: {info['llm_line']}")
        else:
            print("Gemini LLM did not return a valid line number.")
        if info["patch_lines"]:
            print(f"Heuristic patch analysis candidate lines: {info['patch_lines']}")
        else:
            print("No candidate lines found from patch diffs.")

import re
import requests
from datasets import load_dataset
import google.generativeai as genai

# Configure your Gemini API key
genai.configure(api_key="AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA")

import re
import requests
from datasets import load_dataset
import google.generativeai as genai

# Configure your Gemini API key (use a valid key)
genai.configure(api_key="AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA")

def extract_file_diff(patch_text, file_path):
    """
    Splits the patch text by "diff --git" markers and returns the diff
    corresponding to the target file. Checks both full path and filename.
    """
    file_diffs = patch_text.split("diff --git")
    for diff in file_diffs:
        if file_path in diff or file_path.split("/")[-1] in diff:
            return "diff --git" + diff  # add back the marker
    return None

def extract_method_patch_snippet(file_diff, method_name):
    """
    Splits the file diff into hunks and returns the first hunk that contains the method name.
    Annotates added lines with absolute line numbers.

    The diff is split such that hunk headers are at odd indices and hunk bodies at even indices.
    """
    if not file_diff:
        return None

    parts = re.split(r"(@@.*?@@)", file_diff)
    # parts[0] is the prefix; hunk headers are at indices 1, 3, 5, etc.
    for i in range(1, len(parts) - 1, 2):
        hunk_header = parts[i].strip()      # header (e.g., @@ -128,7 +134,12 @@ ...)
        hunk_content = parts[i+1]             # hunk body
        hunk = hunk_header + "\n" + hunk_content
        if method_name in hunk:
            m = re.search(r"\+(\d+),\d+", hunk_header)
            new_line_start = int(m.group(1)) if m else 1
            annotated_lines = []
            current_line = new_line_start
            for line in hunk_content.splitlines():
                if line.startswith("+") and not line.startswith("+++"):
                    annotated_lines.append(f"{current_line}: {line[1:]}")
                    current_line += 1
                elif line.startswith(" ") or (line and not line.startswith("-")):
                    annotated_lines.append(f"{current_line}: {line}")
                    current_line += 1
                elif line.startswith("-") and not line.startswith("---"):
                    annotated_lines.append(f"{current_line}: {line}")
                    # Do not increment for removed lines
                else:
                    annotated_lines.append(line)
            annotated_hunk = hunk_header + "\n" + "\n".join(annotated_lines)
            return annotated_hunk
    # Fallback: if no hunk contains the method, return the first hunk body (with header)
    print("No hunk contains the method; using the first hunk as fallback.")
    if len(parts) >= 3:
        fallback_header = parts[1].strip()
        fallback_body = parts[2]
        return fallback_header + "\n" + fallback_body
    return None

# def find_faulty_line_in_patch(file_path, method_name, dataset):
#     """
#     Searches patch diffs in the dataset for the given file and method.
#     Returns a mapping of PR IDs to candidate modified line numbers.
#     If no explicit candidate is found, uses the hunk header's new line number as fallback.
#     """
#     matching_entries = [
#         entry for entry in dataset["java_verified"]
#         if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]
#     ]

#     faulty_lines_by_pr = {}
#     for entry in matching_entries:
#         patch_text = entry["patch"]
#         file_diff = extract_file_diff(patch_text, file_path)
#         if not file_diff:
#             continue
#         candidate_lines = []
#         current_line = None
#         addition_found = False
#         for line in file_diff.splitlines():
#             if line.startswith("@@"):
#                 m = re.search(r"\+(\d+),\d+", line)
#                 if m:
#                     current_line = int(m.group(1))
#             else:
#                 if current_line is not None:
#                     if line.startswith("+") and not line.startswith("+++"):
#                         addition_found = True
#                         if method_name in line:
#                             candidate_lines.append(current_line)
#                         current_line += 1
#                     elif line.startswith("-") and not line.startswith("---"):
#                         if method_name in line:
#                             candidate_lines.append(current_line)
#                     else:
#                         if not line.startswith("\\"):
#                             current_line += 1
#         if not candidate_lines and addition_found:
#             for line in file_diff.splitlines():
#                 if line.startswith("@@"):
#                     m = re.search(r"\+(\d+),\d+", line)
#                     if m:
#                         candidate_lines.append(int(m.group(1)))
#                         break
#         if candidate_lines:
#             faulty_lines_by_pr[entry["pull_number"]] = candidate_lines
#     return faulty_lines_by_pr
def find_faulty_line_in_patch(file_path, method_name, dataset):
    """
    Searches patch diffs in the dataset for the given file and method.
    Returns a mapping of PR IDs to candidate modified line numbers.
    Uses the hunk header to compute candidate lines relative to the new file.
    """
    matching_entries = [
        entry for entry in dataset["java_verified"]
        if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]
    ]

    faulty_lines_by_pr = {}
    for entry in matching_entries:
        patch_text = entry["patch"]
        file_diff = extract_file_diff(patch_text, file_path)
        if not file_diff:
            continue

        candidate_lines = []
        # Process each hunk separately
        hunks = re.split(r"(@@.*?@@)", file_diff)
        # hunk headers at odd indices
        for i in range(1, len(hunks)-1, 2):
            hunk_header = hunks[i].strip()
            hunk_content = hunks[i+1]
            if method_name not in (hunk_header + hunk_content):
                continue  # skip hunks not related to the method
            m = re.search(r"\+(\d+),\d+", hunk_header)
            if m:
                new_line_start = int(m.group(1))
            else:
                new_line_start = 1
            current_line = new_line_start
            for line in hunk_content.splitlines():
                if line.startswith("+") and not line.startswith("+++"):
                    if method_name in line:
                        candidate_lines.append(current_line)
                    current_line += 1
                elif line.startswith(" ") or (line and not line.startswith("-")):
                    current_line += 1
                # Removed lines are ignored for candidate line calculation in this context
            # If candidate_lines for this hunk is empty, use the hunk's starting line as fallback.
            if not candidate_lines:
                candidate_lines.append(new_line_start)
        if candidate_lines:
            faulty_lines_by_pr[entry["pull_number"]] = candidate_lines
    return faulty_lines_by_pr

def call_gemini_for_fault_line_from_patch(patch_snippet, reason):
    """
    Constructs a prompt using the patch snippet and fault reason,
    then calls the Gemini API (gemini-2.0-flash) via HTTP POST to pinpoint the exact error line.
    Expects the API to return a response containing a candidate with a single integer.
    """
    prompt = f"""Analyze the following Java diff patch snippet (with annotated line numbers):

{patch_snippet}

Reason for fault localization:
{reason}

Please identify and return ONLY the exact line number (absolute line number in the new version of the file) where the error is most likely located.
Return only a single integer.
"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }]
    }
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        print("Gemini API raw result:", result)  # Debug output
        try:
            candidate = result["candidates"][0]
            # Check in candidate content, keys "output" or "text" may be nested inside "content"
            content = candidate.get("output") or candidate.get("text")
            if not content:
                # Try to extract from candidate["content"]["parts"][0]["text"]
                content = candidate.get("content", {}).get("parts", [{}])[0].get("text")
            if content is None:
                print("Neither 'output' nor 'text' found in candidate.")
                return None
            content = content.strip()
            m = re.search(r'\d+', content)
            if m:
                return int(m.group(0))
            else:
                print(f"Unexpected Gemini output format: '{content}'")
        except (KeyError, IndexError, ValueError) as e:
            print("Error parsing Gemini response:", e)
    else:
        print("Gemini API error:", response.status_code, response.text)
    return None

def localize_faulty_line_with_llm(input_data):
    """
    Main pipeline using only dataset patch diffs.
    For each input item (file, class, method, reason):
      1. Filters patches that reference the file and method.
      2. Extracts the file-specific diff and then the diff hunk (patch snippet) with annotated line numbers.
      3. Runs heuristic analysis to extract candidate line numbers.
      4. Queries Gemini with the patch snippet and reason to pinpoint the exact error line.

    Returns a mapping from (file_path, method_name) to the Gemini LLM output and candidate patch lines.
    """
    ds = load_dataset("Daoguang/Multi-SWE-bench")
    results = {}

    for item in input_data:
        file_path = item["file_path"]
        method_name = item["method_name"]
        reason = item.get("reason", "")

        print(f"Processing {file_path} - {method_name}")

        patch_fault_lines = find_faulty_line_in_patch(file_path, method_name, ds)
        if patch_fault_lines:
            print(f"Heuristic candidate lines: {patch_fault_lines}")
        else:
            print("No candidate lines found via heuristic analysis.")

        candidate_snippet = None
        for entry in ds["java_verified"]:
            if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]:
                file_diff = extract_file_diff(entry["patch"], file_path)
                candidate_snippet = extract_method_patch_snippet(file_diff, method_name)
                if candidate_snippet:
                    break

        if candidate_snippet:
            print("Patch snippet for LLM analysis:")
            print(candidate_snippet)
            llm_line = call_gemini_for_fault_line_from_patch(candidate_snippet, reason)
        else:
            print("No patch snippet available for LLM analysis.")
            llm_line = None

        results[(file_path, method_name)] = {"llm_line": llm_line, "patch_lines": patch_fault_lines}

    return results

# --- Example usage ---

input_data = [{
    "file_path": "dubbo-common/src/main/java/org/apache/dubbo/common/utils/CompatibleTypeUtils.java",
    "class_name": "CompatibleTypeUtils",
    "method_name": "compatibleTypeConvert",
    "reason": "The method compatibleTypeConvert had issues handling LocalTime parsing correctly, which could lead to serialization errors."
}]

if __name__ == "__main__":
    results = localize_faulty_line_with_llm(input_data)
    for key, info in results.items():
        file_path, method_name = key
        print(f"\nFile: {file_path}\nMethod: {method_name}")
        if info["llm_line"] is not None:
            print(f"Gemini LLM identified faulty line: {info['llm_line']}")
        else:
            print("Gemini LLM did not return a valid line number.")
        if info["patch_lines"]:
            print(f"Heuristic candidate lines: {info['patch_lines']}")
        else:
            print("No candidate lines found from patch diffs.")

import re
import requests
from datasets import load_dataset
import google.generativeai as genai

# Configure your Gemini API key (ensure you use a valid key)
genai.configure(api_key="AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA")

def extract_file_diff(patch_text, file_path):
    """
    Splits the patch text by "diff --git" markers and returns the diff corresponding
    to the target file. Checks both the full path and filename.
    """
    file_diffs = patch_text.split("diff --git")
    for diff in file_diffs:
        if file_path in diff or file_path.split("/")[-1] in diff:
            return "diff --git" + diff  # add back the marker
    return None

def extract_method_patch_snippet(file_diff, method_name):
    """
    Splits the file diff into hunks and returns the first hunk that contains the method name.
    Annotates added lines with absolute line numbers based on the hunk header.
    """
    if not file_diff:
        return None

    parts = re.split(r"(@@.*?@@)", file_diff)
    # hunk headers are at indices 1, 3, 5, ...
    for i in range(1, len(parts) - 1, 2):
        hunk_header = parts[i].strip()      # e.g. @@ -128,7 +134,12 @@ ...
        hunk_content = parts[i + 1]
        hunk = hunk_header + "\n" + hunk_content
        if method_name in hunk:
            m = re.search(r"\+(\d+),(\d+)", hunk_header)
            if m:
                new_line_start = int(m.group(1))
                line_count = int(m.group(2))
            else:
                new_line_start = 1
                line_count = 1
            annotated_lines = []
            current_line = new_line_start
            for line in hunk_content.splitlines():
                if line.startswith("+") and not line.startswith("+++"):
                    annotated_lines.append(f"{current_line}: {line[1:]}")
                    current_line += 1
                elif line.startswith(" ") or (line and not line.startswith("-")):
                    annotated_lines.append(f"{current_line}: {line}")
                    current_line += 1
                elif line.startswith("-") and not line.startswith("---"):
                    annotated_lines.append(f"{current_line}: {line}")
                    # Do not increment for removed lines
                else:
                    annotated_lines.append(line)
            annotated_hunk = hunk_header + "\n" + "\n".join(annotated_lines)
            return annotated_hunk
    # Fallback: if no hunk contains the method, return the first hunk
    print("No hunk contains the method; using the first hunk as fallback.")
    if len(parts) >= 3:
        fallback_header = parts[1].strip()
        fallback_body = parts[2]
        return fallback_header + "\n" + fallback_body
    return None

def get_hunk_range(annotated_snippet):
    """
    Extracts the hunk's starting line and the number of lines from the annotated snippet header,
    then returns the valid range as a tuple: (start, end)
    """
    m = re.search(r"@@.*\+(\d+),(\d+)", annotated_snippet)
    if m:
        hunk_start = int(m.group(1))
        line_count = int(m.group(2))
        return (hunk_start, hunk_start + line_count - 1)
    return None

def find_faulty_line_in_patch(file_path, method_name, dataset):
    """
    Searches patch diffs in the dataset for the given file and method.
    Returns a mapping of PR IDs to candidate modified line numbers.
    (This heuristic should compute candidate lines relative to the hunk header.)
    """
    matching_entries = [
        entry for entry in dataset["java_verified"]
        if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]
    ]

    faulty_lines_by_pr = {}
    for entry in matching_entries:
        patch_text = entry["patch"]
        file_diff = extract_file_diff(patch_text, file_path)
        if not file_diff:
            continue
        candidate_lines = []
        # Process each hunk in the diff
        hunks = re.split(r"(@@.*?@@)", file_diff)
        for i in range(1, len(hunks)-1, 2):
            hunk_header = hunks[i].strip()
            hunk_body = hunks[i+1]
            if method_name not in (hunk_header + hunk_body):
                continue
            m = re.search(r"\+(\d+),\d+", hunk_header)
            if m:
                new_line_start = int(m.group(1))
            else:
                new_line_start = 1
            current_line = new_line_start
            for line in hunk_body.splitlines():
                if line.startswith("+") and not line.startswith("+++"):
                    if method_name in line:
                        candidate_lines.append(current_line)
                    current_line += 1
                elif line.startswith(" ") or (line and not line.startswith("-")):
                    current_line += 1
            # If no candidate lines were found in this hunk, fallback to the hunk start
            if not candidate_lines:
                candidate_lines.append(new_line_start)
        if candidate_lines:
            faulty_lines_by_pr[entry["pull_number"]] = candidate_lines
    return faulty_lines_by_pr

def parse_llm_candidates(output):
    """
    Parses the Gemini LLM output string for integer candidates.
    Returns a list of integers.
    """
    numbers = re.findall(r'\d+', output)
    return [int(n) for n in numbers] if numbers else []

def call_gemini_for_fault_line_from_patch(patch_snippet, reason):
    """
    Constructs a prompt using the patch snippet and fault reason,
    then calls the Gemini API (gemini-2.0-flash) via HTTP POST to pinpoint the exact error line(s).
    Returns a list of candidate line numbers.
    """
    prompt = f"""Analyze the following Java diff patch snippet (with annotated line numbers):

{patch_snippet}

Reason for fault localization:
{reason}

Please identify and return ONLY the exact line number(s) (absolute line number in the new version of the file) where the error is most likely located.
Return only the integer(s) separated by spaces if there are multiple.
"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }]
    }
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        print("Gemini API raw result:", result)  # Debug output
        try:
            candidate = result["candidates"][0]
            output = candidate.get("output") or candidate.get("text") or candidate.get("content", {}).get("parts", [{}])[0].get("text")
            if output is None:
                print("Neither 'output' nor 'text' found in candidate.")
                return []
            output = output.strip()
            return parse_llm_candidates(output)
        except (KeyError, IndexError, ValueError) as e:
            print("Error parsing Gemini response:", e)
    else:
        print("Gemini API error:", response.status_code, response.text)
    return []

def combine_candidates(heuristic_candidates, llm_candidates, annotated_snippet):
    """
    Combines heuristic and LLM candidates by checking which LLM candidate(s) fall within the hunk range.
    If any LLM candidate is within the range, it is favored. Otherwise, returns LLM candidates if available,
    or falls back to the heuristic candidates.
    """
    hunk_range = get_hunk_range(annotated_snippet)
    if hunk_range:
        hunk_start, hunk_end = hunk_range
        filtered_llm = [num for num in llm_candidates if hunk_start <= num <= hunk_end]
        if filtered_llm:
            return filtered_llm
    return llm_candidates if llm_candidates else heuristic_candidates

def localize_faulty_line_with_llm(input_data):
    """
    Main pipeline using only dataset patch diffs.
    For each input item (file, class, method, reason):
      1. Filters patches that reference the file and method.
      2. Extracts the file-specific diff and then the diff hunk (patch snippet) with annotated line numbers.
      3. Runs heuristic analysis to extract candidate line numbers.
      4. Queries Gemini with the patch snippet and reason to pinpoint the exact error line(s).
      5. Combines the heuristic and LLM candidates by ensuring LLM candidates fall within the hunk range.

    Returns a mapping from (file_path, method_name) to the final candidate line(s)
    and the raw outputs for debugging.
    """
    ds = load_dataset("Daoguang/Multi-SWE-bench")
    results = {}

    for item in input_data:
        file_path = item["file_path"]
        method_name = item["method_name"]
        reason = item.get("reason", "")

        print(f"Processing {file_path} - {method_name}")

        heuristic_candidates = find_faulty_line_in_patch(file_path, method_name, ds)
        if heuristic_candidates:
            print(f"Heuristic candidate lines: {heuristic_candidates}")
        else:
            print("No candidate lines found via heuristic analysis.")

        candidate_snippet = None
        for entry in ds["java_verified"]:
            if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]:
                file_diff = extract_file_diff(entry["patch"], file_path)
                candidate_snippet = extract_method_patch_snippet(file_diff, method_name)
                if candidate_snippet:
                    break

        if candidate_snippet:
            print("Patch snippet for LLM analysis:")
            print(candidate_snippet)
            llm_candidates = call_gemini_for_fault_line_from_patch(candidate_snippet, reason)
        else:
            print("No patch snippet available for LLM analysis.")
            llm_candidates = []

        final_candidates = combine_candidates(
            heuristic_candidates.get(list(heuristic_candidates.keys())[0], []) if heuristic_candidates else [],
            llm_candidates,
            candidate_snippet if candidate_snippet else ""
        )

        results[(file_path, method_name)] = {
            "final_candidates": final_candidates,
            "llm_candidates": llm_candidates,
            "heuristic_candidates": heuristic_candidates
        }

    return results

# --- Example usage ---

input_data = [{
    "file_path": "dubbo-common/src/main/java/org/apache/dubbo/common/utils/CompatibleTypeUtils.java",
    "class_name": "CompatibleTypeUtils",
    "method_name": "compatibleTypeConvert",
    "reason": "The method compatibleTypeConvert had issues handling LocalTime parsing correctly, which could lead to serialization errors."
}]

if __name__ == "__main__":
    results = localize_faulty_line_with_llm(input_data)
    for key, info in results.items():
        file_path, method_name = key
        print(f"\nFile: {file_path}\nMethod: {method_name}")
        if info["final_candidates"]:
            print(f"Final candidate line(s): {info['final_candidates']}")
        else:
            print("No final candidate lines determined.")
        print(f"LLM candidates: {info['llm_candidates']}")
        print(f"Heuristic candidate lines: {info['heuristic_candidates']}")

import re
import requests
from datasets import load_dataset
import google.generativeai as genai

# Configure your Gemini API key (ensure you use a valid key)
genai.configure(api_key="AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA")

def extract_file_diff(patch_text, file_path):
    """
    Splits the patch text by "diff --git" markers and returns the diff corresponding
    to the target file. Checks both the full path and filename.
    """
    file_diffs = patch_text.split("diff --git")
    for diff in file_diffs:
        if file_path in diff or file_path.split("/")[-1] in diff:
            return "diff --git" + diff  # add back the marker
    return None

def extract_method_patch_snippet(file_diff, method_name):
    """
    Splits the file diff into hunks and returns the first hunk that contains the method name.
    Annotates added lines with absolute line numbers based on the hunk header.
    """
    if not file_diff:
        return None

    parts = re.split(r"(@@.*?@@)", file_diff)
    # hunk headers are at indices 1, 3, 5, ...
    for i in range(1, len(parts) - 1, 2):
        hunk_header = parts[i].strip()      # e.g. @@ -128,7 +134,12 @@ ...
        hunk_content = parts[i + 1]
        hunk = hunk_header + "\n" + hunk_content
        if method_name in hunk:
            m = re.search(r"\+(\d+),(\d+)", hunk_header)
            if m:
                new_line_start = int(m.group(1))
                line_count = int(m.group(2))
            else:
                new_line_start = 1
                line_count = 1
            annotated_lines = []
            current_line = new_line_start
            for line in hunk_content.splitlines():
                if line.startswith("+") and not line.startswith("+++"):
                    annotated_lines.append(f"{current_line}: {line[1:]}")
                    current_line += 1
                elif line.startswith(" ") or (line and not line.startswith("-")):
                    annotated_lines.append(f"{current_line}: {line}")
                    current_line += 1
                elif line.startswith("-") and not line.startswith("---"):
                    annotated_lines.append(f"{current_line}: {line}")
                    # Do not increment for removed lines
                else:
                    annotated_lines.append(line)
            annotated_hunk = hunk_header + "\n" + "\n".join(annotated_lines)
            return annotated_hunk
    # Fallback: if no hunk contains the method, return the first hunk
    print("No hunk contains the method; using the first hunk as fallback.")
    if len(parts) >= 3:
        fallback_header = parts[1].strip()
        fallback_body = parts[2]
        return fallback_header + "\n" + fallback_body
    return None

def get_hunk_range(annotated_snippet):
    """
    Extracts the hunk's starting line number and the number of lines from the annotated snippet's header,
    and returns the valid range as a tuple: (start, end).
    """
    m = re.search(r"@@.*\+(\d+),(\d+)", annotated_snippet)
    if m:
        hunk_start = int(m.group(1))
        line_count = int(m.group(2))
        return (hunk_start, hunk_start + line_count - 1)
    return None

def get_line_content_from_snippet(annotated_snippet, candidate_line):
    """
    Given an annotated snippet (with lines like '134: code...'),
    returns the content of the line corresponding to candidate_line.
    """
    for line in annotated_snippet.splitlines():
        # Look for a line that starts with the candidate number followed by a colon.
        if re.match(rf"^\s*{candidate_line}\s*:", line):
            # Return the content after the colon
            parts = line.split(":", 1)
            if len(parts) > 1:
                return parts[1].strip()
    return None

def find_faulty_line_in_patch(file_path, method_name, dataset):
    """
    Searches patch diffs in the dataset for the given file and method.
    Returns a mapping of PR IDs to candidate modified line numbers.
    Uses the hunk header to compute candidate lines relative to the new file.
    """
    matching_entries = [
        entry for entry in dataset["java_verified"]
        if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]
    ]

    faulty_lines_by_pr = {}
    for entry in matching_entries:
        patch_text = entry["patch"]
        file_diff = extract_file_diff(patch_text, file_path)
        if not file_diff:
            continue
        candidate_lines = []
        # Process each hunk in the diff
        hunks = re.split(r"(@@.*?@@)", file_diff)
        for i in range(1, len(hunks)-1, 2):
            hunk_header = hunks[i].strip()
            hunk_body = hunks[i+1]
            if method_name not in (hunk_header + hunk_body):
                continue
            m = re.search(r"\+(\d+),\d+", hunk_header)
            if m:
                new_line_start = int(m.group(1))
            else:
                new_line_start = 1
            current_line = new_line_start
            for line in hunk_body.splitlines():
                if line.startswith("+") and not line.startswith("+++"):
                    if method_name in line:
                        candidate_lines.append(current_line)
                    current_line += 1
                elif line.startswith(" ") or (line and not line.startswith("-")):
                    current_line += 1
            # Fallback if no candidate found in this hunk:
            if not candidate_lines:
                candidate_lines.append(new_line_start)
        if candidate_lines:
            faulty_lines_by_pr[entry["pull_number"]] = candidate_lines
    return faulty_lines_by_pr

def parse_llm_candidates(output):
    """
    Parses the Gemini LLM output string for integer candidates.
    Returns a list of integers.
    """
    numbers = re.findall(r'\d+', output)
    return [int(n) for n in numbers] if numbers else []

def call_gemini_for_fault_line_from_patch(patch_snippet, reason):
    """
    Constructs a prompt using the patch snippet and fault reason,
    then calls the Gemini API (gemini-2.0-flash) via HTTP POST to pinpoint the exact error line(s).
    Returns a list of candidate line numbers.
    """
    prompt = f"""Analyze the following Java diff patch snippet (with annotated line numbers):

{patch_snippet}

Reason for fault localization:
{reason}

Please identify and return ONLY the exact line number(s) (absolute line number in the new version of the file) where the error is most likely located.
Return only the integer(s) separated by spaces if there are multiple.
"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }]
    }
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        print("Gemini API raw result:", result)  # Debug output
        try:
            candidate = result["candidates"][0]
            output = candidate.get("output") or candidate.get("text") or candidate.get("content", {}).get("parts", [{}])[0].get("text")
            if output is None:
                print("Neither 'output' nor 'text' found in candidate.")
                return []
            output = output.strip()
            return parse_llm_candidates(output)
        except (KeyError, IndexError, ValueError) as e:
            print("Error parsing Gemini response:", e)
    else:
        print("Gemini API error:", response.status_code, response.text)
    return []

def combine_candidates(heuristic_candidates, llm_candidates, annotated_snippet):
    """
    Combines heuristic and LLM candidates by checking which LLM candidate(s) fall within the hunk range.
    If any LLM candidate is within the range, that candidate is favored.
    Otherwise, returns LLM candidates if available; if not, returns the heuristic candidates.
    """
    hunk_range = get_hunk_range(annotated_snippet)
    if hunk_range:
        hunk_start, hunk_end = hunk_range
        filtered_llm = [num for num in llm_candidates if hunk_start <= num <= hunk_end]
        if filtered_llm:
            return filtered_llm
    return llm_candidates if llm_candidates else heuristic_candidates

def localize_faulty_line_with_llm(input_data):
    """
    Main pipeline using only dataset patch diffs.
    For each input item (file, class, method, reason):
      1. Filters patches that reference the file and method.
      2. Extracts the file-specific diff and then the diff hunk (patch snippet) with annotated line numbers.
      3. Runs heuristic analysis to extract candidate line numbers.
      4. Queries Gemini with the patch snippet and reason to pinpoint the exact error line(s).
      5. Combines the heuristic and LLM candidates by ensuring LLM candidates fall within the hunk range.
      6. Extracts the content of each candidate line from the annotated snippet.

    Returns a mapping from (file_path, method_name) to the final candidate line numbers,
    their corresponding content, and raw outputs for debugging.
    """
    ds = load_dataset("Daoguang/Multi-SWE-bench")
    results = {}

    for item in input_data:
        file_path = item["file_path"]
        method_name = item["method_name"]
        reason = item.get("reason", "")

        print(f"Processing {file_path} - {method_name}")

        heuristic_candidates = find_faulty_line_in_patch(file_path, method_name, ds)
        if heuristic_candidates:
            print(f"Heuristic candidate lines: {heuristic_candidates}")
        else:
            print("No candidate lines found via heuristic analysis.")

        candidate_snippet = None
        for entry in ds["java_verified"]:
            if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]:
                file_diff = extract_file_diff(entry["patch"], file_path)
                candidate_snippet = extract_method_patch_snippet(file_diff, method_name)
                if candidate_snippet:
                    break

        if candidate_snippet:
            print("Patch snippet for LLM analysis:")
            print(candidate_snippet)
            llm_candidates = call_gemini_for_fault_line_from_patch(candidate_snippet, reason)
        else:
            print("No patch snippet available for LLM analysis.")
            llm_candidates = []

        final_candidates = combine_candidates(
            heuristic_candidates.get(list(heuristic_candidates.keys())[0], []) if heuristic_candidates else [],
            llm_candidates,
            candidate_snippet if candidate_snippet else ""
        )

        # Now extract the content of each candidate line from the annotated snippet.
        candidate_contents = {}
        if candidate_snippet:
            for num in final_candidates:
                content = get_line_content_from_snippet(candidate_snippet, num)
                candidate_contents[num] = content

        results[(file_path, method_name)] = {
            "final_candidate_line_numbers": final_candidates,
            "final_candidate_contents": candidate_contents,
            "llm_candidates": llm_candidates,
            "heuristic_candidates": heuristic_candidates
        }

    return results

def get_line_content_from_snippet(annotated_snippet, candidate_line):
    """
    Given an annotated snippet (with lines like '134: code...'),
    returns the content of the line corresponding to candidate_line.
    """
    for line in annotated_snippet.splitlines():
        # Look for lines that start with the candidate line number followed by a colon.
        if re.match(rf"^\s*{candidate_line}\s*:", line):
            parts = line.split(":", 1)
            if len(parts) > 1:
                return parts[1].strip()
    return None

# --- Example usage ---

input_data = [{
    "file_path": "dubbo-common/src/main/java/org/apache/dubbo/common/utils/CompatibleTypeUtils.java",
    "class_name": "CompatibleTypeUtils",
    "method_name": "compatibleTypeConvert",
    "reason": "The method compatibleTypeConvert had issues handling LocalTime parsing correctly, which could lead to serialization errors."
}]

if __name__ == "__main__":
    results = localize_faulty_line_with_llm(input_data)
    for key, info in results.items():
        file_path, method_name = key
        print(f"\nFile: {file_path}\nMethod: {method_name}")
        if info["final_candidate_line_numbers"]:
            print(f"Final candidate line numbers: {info['final_candidate_line_numbers']}")
            print("Corresponding candidate line contents:")
            for ln, content in info["final_candidate_contents"].items():
                print(f"  Line {ln}: {content}")
        else:
            print("No final candidate lines determined.")
        print(f"LLM candidates: {info['llm_candidates']}")
        print(f"Heuristic candidate lines: {info['heuristic_candidates']}")

from google.colab import files
uploaded = files.upload()

!pip install datasets

import zipfile
import json

input_data = []

with zipfile.ZipFile("test_output.zip", 'r') as zip_ref:
    for file in zip_ref.namelist():
        # ⛔ skip macOS metadata files
        if file.startswith("__MACOSX") or "/._" in file or file.endswith(".DS_Store"):
            continue
        if file.endswith(".json"):
            with zip_ref.open(file) as f:
                try:
                    data = json.load(f)
                    input_data.extend(data)
                except Exception as e:
                    print(f"❌ Error reading {file}: {e}")

import re
import requests
from collections import Counter
from datasets import load_dataset
import google.generativeai as genai

# Configure your Gemini API key (ensure you use a valid key)
genai.configure(api_key="AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA")

def extract_file_diff(patch_text, file_path):
    """
    Splits the patch text by "diff --git" markers and returns the diff corresponding
    to the target file. Checks both the full path and filename.
    """
    file_diffs = patch_text.split("diff --git")
    for diff in file_diffs:
        if file_path in diff or file_path.split("/")[-1] in diff:
            return "diff --git" + diff  # add back the marker
    return None

# def extract_method_patch_snippet(file_diff, method_name):
#     """
#     Splits the file diff into hunks and returns the first hunk that contains the method name.
#     Annotates added lines with absolute line numbers based on the hunk header.
#     """
#     if not file_diff:
#         return None

#     parts = re.split(r"(@@.*?@@)", file_diff)
#     # hunk headers are at indices 1, 3, 5, ...
#     for i in range(1, len(parts) - 1, 2):
#         hunk_header = parts[i].strip()      # e.g. @@ -128,7 +134,12 @@ ...
#         hunk_content = parts[i+1]
#         hunk = hunk_header + "\n" + hunk_content
#         if method_name in hunk:
#             m = re.search(r"\+(\d+),(\d+)", hunk_header)
#             if m:
#                 new_line_start = int(m.group(1))
#                 line_count = int(m.group(2))
#             else:
#                 new_line_start = 1
#                 line_count = 1
#             annotated_lines = []
#             current_line = new_line_start
#             for line in hunk_content.splitlines():
#                 if line.startswith("+") and not line.startswith("+++"):
#                     annotated_lines.append(f"{current_line}: {line[1:]}")
#                     current_line += 1
#                 elif line.startswith(" ") or (line and not line.startswith("-")):
#                     annotated_lines.append(f"{current_line}: {line}")
#                     current_line += 1
#                 elif line.startswith("-") and not line.startswith("---"):
#                     annotated_lines.append(f"{current_line}: {line}")
#                     # Do not increment for removed lines
#                 else:
#                     annotated_lines.append(line)
#             annotated_hunk = hunk_header + "\n" + "\n".join(annotated_lines)
#             return annotated_hunk
#     # Fallback: if no hunk contains the method, return the first hunk
#     print("No hunk contains the method; using the first hunk as fallback.")
#     if len(parts) >= 3:
#         fallback_header = parts[1].strip()
#         fallback_body = parts[2]
#         return fallback_header + "\n" + fallback_body
#     return None
def extract_method_patch_snippet(file_diff, method_name=None):
    """
    Returns the first hunk that contains the method name (if given),
    otherwise returns the first added hunk.
    """
    if not file_diff:
        return None

    parts = re.split(r"(@@.*?@@)", file_diff)
    for i in range(1, len(parts) - 1, 2):
        hunk_header = parts[i].strip()
        hunk_content = parts[i + 1]
        hunk = hunk_header + "\n" + hunk_content

        if method_name and method_name not in hunk:
            continue  # skip if method provided but not matched

        # If method is matched or no method filter, annotate this hunk
        m = re.search(r"\+(\d+),(\d+)", hunk_header)
        new_line_start = int(m.group(1)) if m else 1
        current_line = new_line_start

        annotated_lines = []
        for line in hunk_content.splitlines():
            if line.startswith("+") and not line.startswith("+++"):
                annotated_lines.append(f"{current_line}: {line[1:]}")
                current_line += 1
            elif line.startswith(" ") or (line and not line.startswith("-")):
                annotated_lines.append(f"{current_line}: {line}")
                current_line += 1
            elif line.startswith("-") and not line.startswith("---"):
                annotated_lines.append(f"{current_line}: {line}")
            else:
                annotated_lines.append(line)

        annotated_hunk = hunk_header + "\n" + "\n".join(annotated_lines)
        return annotated_hunk

    # No matching hunk — fallback to first hunk
    print("No matching method hunk; using the first hunk as fallback.")
    if len(parts) >= 3:
        fallback_header = parts[1].strip()
        fallback_body = parts[2]
        return fallback_header + "\n" + fallback_body
    return None

def get_hunk_range(annotated_snippet):
    """
    Extracts the hunk's starting line number and the number of lines from the annotated snippet's header,
    and returns the valid range as a tuple: (start, end).
    """
    m = re.search(r"@@.*\+(\d+),(\d+)", annotated_snippet)
    if m:
        hunk_start = int(m.group(1))
        line_count = int(m.group(2))
        return (hunk_start, hunk_start + line_count - 1)
    return None

def get_line_content_from_snippet(annotated_snippet, candidate_line):
    """
    Given an annotated snippet (with lines like '134: code...'),
    returns the content of the line corresponding to candidate_line.
    """
    for line in annotated_snippet.splitlines():
        if re.match(rf"^\s*{candidate_line}\s*:", line):
            parts = line.split(":", 1)
            if len(parts) > 1:
                return parts[1].strip()
    return None

# def find_faulty_line_in_patch(file_path, method_name, dataset):
#     """
#     Searches patch diffs in the dataset for the given file and method.
#     Returns a mapping of PR IDs to candidate modified line numbers using hunk header information.
#     """
#     matching_entries = [
#         entry for entry in dataset["java_verified"]
#         if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]
#     ]

#     faulty_lines_by_pr = {}
#     for entry in matching_entries:
#         patch_text = entry["patch"]
#         file_diff = extract_file_diff(patch_text, file_path)
#         if not file_diff:
#             continue
#         candidate_lines = []
#         hunks = re.split(r"(@@.*?@@)", file_diff)
#         for i in range(1, len(hunks)-1, 2):
#             hunk_header = hunks[i].strip()
#             hunk_body = hunks[i+1]
#             if method_name not in (hunk_header + hunk_body):
#                 continue
#             m = re.search(r"\+(\d+),\d+", hunk_header)
#             if m:
#                 new_line_start = int(m.group(1))
#             else:
#                 new_line_start = 1
#             current_line = new_line_start
#             for line in hunk_body.splitlines():
#                 if line.startswith("+") and not line.startswith("+++"):
#                     if method_name in line:
#                         candidate_lines.append(current_line)
#                     current_line += 1
#                 elif line.startswith(" ") or (line and not line.startswith("-")):
#                     current_line += 1
#             if not candidate_lines:
#                 candidate_lines.append(new_line_start)
#         if candidate_lines:
#             faulty_lines_by_pr[entry["pull_number"]] = candidate_lines
#     return faulty_lines_by_pr
def find_faulty_line_in_patch(file_path, method_name, dataset):
    """
    Extracts added lines from a patch, optionally scoped to a method.
    """
    matching_entries = [
        entry for entry in dataset["java_verified"]
        if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"])
    ]

    faulty_lines_by_pr = {}
    for entry in matching_entries:
        patch_text = entry["patch"]
        file_diff = extract_file_diff(patch_text, file_path)
        if not file_diff:
            continue

        candidate_lines = []
        hunks = re.split(r"(@@.*?@@)", file_diff)
        for i in range(1, len(hunks) - 1, 2):
            hunk_header = hunks[i].strip()
            hunk_body = hunks[i + 1]

            # Skip this hunk only if method_name is provided and not matched
            if method_name and method_name not in (hunk_header + hunk_body):
                continue

            m = re.search(r"\+(\d+),\d+", hunk_header)
            new_line_start = int(m.group(1)) if m else 1
            current_line = new_line_start

            for line in hunk_body.splitlines():
                if line.startswith("+") and not line.startswith("+++"):
                    candidate_lines.append(current_line)
                    current_line += 1
                elif line.startswith(" ") or (line and not line.startswith("-")):
                    current_line += 1

        if candidate_lines:
            faulty_lines_by_pr[entry["pull_number"]] = candidate_lines

    return faulty_lines_by_pr

def parse_llm_candidates(output):
    """
    Parses the Gemini LLM output string for integer candidates.
    Returns a list of integers.
    """
    numbers = re.findall(r'\d+', output)
    return [int(n) for n in numbers] if numbers else []

def call_gemini_for_fault_line_from_patch(patch_snippet, reason, num_samples=5):
    """
    Calls the Gemini API multiple times to reduce variance.
    Returns an aggregated list of candidate line numbers.
    """
    prompt = f"""Analyze the following Java diff patch snippet (with annotated line numbers):

{patch_snippet}

Reason for fault localization:
{reason}

Please identify and return ONLY the exact line number(s) (absolute line number in the new version of the file) where the error is most likely located.
Return only the integer(s) separated by spaces if there are multiple.
"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBDCDHP0ljOpPfZGTbwIZv04EIvi_R5BCA"
    headers = {"Content-Type": "application/json"}
    candidate_lists = []
    for _ in range(num_samples):
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }
        # response = requests.post(url, headers=headers, json=payload)
        # if response.status_code == 200:
        #     result = response.json()
        #     try:
        #         candidate = result["candidates"][0]
        #         output = candidate.get("output") or candidate.get("text") or candidate.get("content", {}).get("parts", [{}])[0].get("text")
        #         if output is None:
        #             print("Neither 'output' nor 'text' found in candidate.")
        #             continue
        #         output = output.strip()
        #         candidate_lines = parse_llm_candidates(output)
        #         candidate_lists.append(candidate_lines)
        #     except (KeyError, IndexError, ValueError) as e:
        #         print("Error parsing Gemini response:", e)
        # else:
        #     print("Gemini API error:", response.status_code, response.text)
        import time

        MAX_RETRIES = 5
        RETRY_DELAY = 60  # seconds

        for _ in range(num_samples):
            retries = 0
            while retries < MAX_RETRIES:
                response = requests.post(url, headers=headers, json=payload)
                if response.status_code == 200:
                    result = response.json()
                    try:
                        candidate = result["candidates"][0]
                        output = candidate.get("output") or candidate.get("text") or candidate.get("content", {}).get("parts", [{}])[0].get("text")
                        if output is None:
                            print("No valid output found.")
                            break
                        output = output.strip()
                        candidate_lines = parse_llm_candidates(output)
                        candidate_lists.append(candidate_lines)
                        break  # success
                    except (KeyError, IndexError, ValueError) as e:
                        print("Error parsing Gemini response:", e)
                        break
                elif response.status_code == 429:
                    print(f"Rate limited. Retrying in {RETRY_DELAY} seconds...")
                    time.sleep(RETRY_DELAY)
                    retries += 1
                else:
                    print("Gemini API error:", response.status_code, response.text)
                    break
    # Flatten and aggregate the candidates
    all_candidates = [num for sublist in candidate_lists for num in sublist]
    if not all_candidates:
        return []
    counts = Counter(all_candidates)
    max_count = max(counts.values())
    aggregated = sorted([num for num, count in counts.items() if count == max_count])
    return aggregated

def combine_candidates(heuristic_candidates, llm_candidates, annotated_snippet):
    """
    Combines heuristic and LLM candidates by checking which LLM candidate(s) fall within the hunk range.
    If any LLM candidate is within the range, it is favored.
    Otherwise, returns LLM candidates if available; if not, returns the heuristic candidates.
    """
    hunk_range = get_hunk_range(annotated_snippet)
    if hunk_range:
        hunk_start, hunk_end = hunk_range
        filtered_llm = [num for num in llm_candidates if hunk_start <= num <= hunk_end]
        if filtered_llm:
            return filtered_llm
    return llm_candidates if llm_candidates else heuristic_candidates

def localize_faulty_line_with_llm(input_data):
    """
    Main pipeline using only dataset patch diffs.
    For each input item (file, class, method, reason):
      1. Filters patches that reference the file and method.
      2. Extracts the file-specific diff and then the diff hunk (patch snippet) with annotated line numbers.
      3. Runs heuristic analysis to extract candidate line numbers.
      4. Queries Gemini multiple times with the patch snippet and reason to pinpoint the exact error line(s).
      5. Combines the heuristic and LLM candidates by ensuring LLM candidates fall within the hunk range.
      6. Extracts the content of each candidate line from the annotated snippet.

    Returns a mapping from (file_path, method_name) to the final candidate line numbers,
    their corresponding content, and raw outputs for debugging.
    """
    ds = load_dataset("Daoguang/Multi-SWE-bench")
    results = {}

    for item in input_data:
        file_path = item["file_path"]
        method_name = item["method_name"]
        reason = item.get("reason", "")

        print(f"Processing {file_path} - {method_name}")

        heuristic_candidates = find_faulty_line_in_patch(file_path, method_name, ds)
        if heuristic_candidates:
            print(f"Heuristic candidate lines: {heuristic_candidates}")
        else:
            print("No candidate lines found via heuristic analysis.")

        candidate_snippet = None
        for entry in ds["java_verified"]:
            if (file_path in entry["patch"] or file_path.split("/")[-1] in entry["patch"]) and method_name in entry["patch"]:
                file_diff = extract_file_diff(entry["patch"], file_path)
                candidate_snippet = extract_method_patch_snippet(file_diff, method_name)
                if candidate_snippet:
                    break

        if candidate_snippet:
            print("Patch snippet for LLM analysis:")
            print(candidate_snippet)
            llm_candidates = call_gemini_for_fault_line_from_patch(candidate_snippet, reason)
        else:
            print("No patch snippet available for LLM analysis.")
            llm_candidates = []

        final_candidates = combine_candidates(
            heuristic_candidates.get(list(heuristic_candidates.keys())[0], []) if heuristic_candidates else [],
            llm_candidates,
            candidate_snippet if candidate_snippet else ""
        )

        candidate_contents = {}
        if candidate_snippet:
            for num in final_candidates:
                content = get_line_content_from_snippet(candidate_snippet, num)
                candidate_contents[num] = content

        results[(file_path, method_name)] = {
            "final_candidate_line_numbers": final_candidates,
            "final_candidate_contents": candidate_contents,
            "llm_candidates": llm_candidates,
            "heuristic_candidates": heuristic_candidates
        }

    return results

results = localize_faulty_line_with_llm(input_data)

import re
from datasets import load_dataset

def extract_added_lines_from_patch(patch_text):
    """
    Parses a unified diff and returns a dictionary:
    {
      file_path: [list of added line numbers]
    }
    """
    file_to_lines = {}
    patch_blocks = patch_text.split("diff --git ")

    for block in patch_blocks:
        if not block.strip():
            continue

        lines = block.strip().splitlines()

        # Find the file path (from the diff --git line)
        if not lines:
            continue
        header = lines[0]  # e.g., "a/foo.java b/foo.java"
        match = re.match(r"a\/(.+?)\s+b\/(.+)", header)
        if not match:
            continue
        file_path = match.group(2).strip()  # Get 'b/...' path

        # Process hunks
        hunk_blocks = re.split(r"(@@.*?@@)", block)
        for i in range(1, len(hunk_blocks), 2):
            hunk_header = hunk_blocks[i]
            hunk_body = hunk_blocks[i + 1] if i + 1 < len(hunk_blocks) else ""

            hunk_match = re.search(r"\+(\d+)(?:,(\d+))?", hunk_header)
            if not hunk_match:
                continue

            start_line = int(hunk_match.group(1))
            current_line = start_line

            for line in hunk_body.splitlines():
                if line.startswith('+') and not line.startswith('+++'):
                    file_to_lines.setdefault(file_path, []).append(current_line)
                    current_line += 1
                elif line.startswith(' ') or (line and not line.startswith('-')):
                    current_line += 1
                # Removed lines do not increment line number

    return file_to_lines

def generate_ground_truth_line_level():
    """
    Returns a dict in format:
    {
      (file_path, None): [list of added lines]
    }
    which can be directly compared to your model's predictions.
    """
    dataset = load_dataset("Daoguang/Multi-SWE-bench")
    ground_truth = {}

    for entry in dataset["java_verified"]:
        patch = entry["patch"]
        added_lines_per_file = extract_added_lines_from_patch(patch)
        for file_path, line_numbers in added_lines_per_file.items():
            key = (file_path, None)  # Set method_name to None for now
            if key not in ground_truth:
                ground_truth[key] = set()
            ground_truth[key].update(line_numbers)

    # Convert sets to sorted lists
    for key in ground_truth:
        ground_truth[key] = sorted(ground_truth[key])

    print(f"Generated ground truth for {len(ground_truth)} (file, None) keys.")
    return ground_truth

ground_truth = generate_ground_truth_line_level()

# For evaluation: compare
for (file_path, _), result in results.items():
    predicted = result["final_candidate_line_numbers"]
    truth = ground_truth.get((file_path, None), [])
    print(f"{file_path} — Predicted: {predicted}, Ground Truth: {truth}")

def evaluate_fault_localization(results, ground_truth):
    """
    Evaluates line-level fault localization.

    Args:
        results (dict): Output from your `localize_faulty_line_with_llm()` function.
                        Keys are (file_path, method_name)
                        Values are dicts with key 'final_candidate_line_numbers'
        ground_truth (dict): Keys are (file_path, None)
                             Values are lists of actual fixed line numbers

    Returns:
        metrics (dict): Aggregated metrics (precision, recall, F1)
        instance_stats (list): Detailed stats per instance
    """
    total_tp = 0
    total_fp = 0
    total_fn = 0

    instance_stats = []

    for (file_path, method_name), result in results.items():
        pred_lines = set(result.get("final_candidate_line_numbers", []))
        true_lines = set(ground_truth.get((file_path, None), []))

        tp = len(pred_lines & true_lines)
        fp = len(pred_lines - true_lines)
        fn = len(true_lines - pred_lines)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        total_tp += tp
        total_fp += fp
        total_fn += fn

        instance_stats.append({
            "file": file_path,
            "method": method_name,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "tp": tp,
            "fp": fp,
            "fn": fn,
            "predicted_lines": sorted(pred_lines),
            "ground_truth_lines": sorted(true_lines)
        })

    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0

    metrics = {
        "total_instances": len(results),
        "precision": round(overall_precision, 4),
        "recall": round(overall_recall, 4),
        "f1": round(overall_f1, 4),
        "true_positives": total_tp,
        "false_positives": total_fp,
        "false_negatives": total_fn
    }

    return metrics, instance_stats

metrics, details = evaluate_fault_localization(results, ground_truth)

print("=== Evaluation Metrics ===")
for k, v in metrics.items():
    print(f"{k}: {v}")

# Optionally show a few sample stats
print("\n=== Sample Instance Evaluation ===")
for stat in details[:5]:  # show 5 samples
    print(stat)







import pandas as pd
import json
import os

def process_and_store(results_dict, df_output_path=None, json_output_path=None):
    """
    Takes in a dictionary of results (e.g., from localize_faulty_line_with_llm),
    converts them to a DataFrame, and optionally saves them to CSV (or JSON)
    and/or writes them to a JSON file.

    Args:
        results_dict (dict): A dictionary structured like:
            {
              (file_path, method_name): {
                "final_candidate_line_numbers": [...],
                "final_candidate_contents": {...},
                "llm_candidates": [...],
                "heuristic_candidates": {...}
              },
              ...
            }
        df_output_path (str): If provided, save DataFrame version to this CSV path.
        json_output_path (str): If provided, save JSON version to this path.
    """
    # Convert your results into a list of dicts so it can become a DataFrame.
    # Example structure: each row is a file-method pair, plus the result fields.
    data_for_df = []
    for (file_path, method_name), info in results_dict.items():
        row = {
            "file_path": file_path,
            "method_name": method_name,
            "final_candidate_line_numbers": info["final_candidate_line_numbers"],
            "final_candidate_contents": info["final_candidate_contents"],
            "llm_candidates": info["llm_candidates"],
            "heuristic_candidates": info["heuristic_candidates"]
        }
        data_for_df.append(row)

    # Create a DataFrame.
    df = pd.DataFrame(data_for_df)

    # Optionally save DataFrame to CSV (or any other format supported by pandas).
    if df_output_path:
        df.to_csv(df_output_path, index=False)
        print(f"Results saved to CSV: {df_output_path}")

    # Optionally write the entire dictionary to JSON if you prefer that format.
    if json_output_path:
        # Ensure the directory exists
        os.makedirs(os.path.dirname(os.path.abspath(json_output_path)), exist_ok=True)
        with open(json_output_path, 'w') as f:
            json.dump(results_dict, f, indent=2)
        print(f"Results saved to JSON: {json_output_path}")

    # Return the DataFrame in case you want to inspect it in your Python environment.
    return df

# Usage Example:
if __name__ == "__main__":
    # Suppose 'results' is what you get from localize_faulty_line_with_llm(...)


    # Convert to DataFrame and optionally save to files
    df_results = process_and_store(
        results,
        df_output_path="out/results.csv",
        json_output_path="out/results.json"
    )

    # You can now inspect 'df_results' or load 'out/results.json' in your pipeline.
