# -*- coding: utf-8 -*-
"""Copy of Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HgLSZVI8cMUc1GhmCBKJaadYK3toCq9R
"""

# --- Cell 1: Install Dependencies (Run once per session) ---
!pip install datasets tqdm "openai>=1.0.0" tiktoken  # 加上 openai>=1.0
!apt-get update && apt-get install -y git

import openai
print(openai.__version__)

# -*- coding: utf-8 -*-
"""
repair_colab_standalone_v7.py

Standalone script for Java code repair in Google Colab.
- Loads Ground Truth from Hugging Face dataset.
- Loads Localization results (detailed_locations) from an uploaded file.
- Uses OpenAI API.
- Uses SEARCH/REPLACE diff format.
- Includes utility functions. Implements Git interactions. LLM Client needs keys.
"""

# --- Standard Libraries ---
import json
import os
import re
import subprocess
import shutil
import tempfile
import time
from difflib import unified_diff
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Tuple, Any # Added more type hints

# --- Required Libraries (Install in Colab) ---
try:
    from datasets import load_dataset
except ImportError:
    print("ERROR: 'datasets' library not found. Please install using: !pip install datasets")
    load_dataset = None

try:
    from tqdm.notebook import tqdm
except ImportError:
    print("Warning: 'tqdm' library not found. Install using: !pip install tqdm")
    def tqdm(iterable, *args, **kwargs): return iterable

try:
    import openai
    # Import specific exceptions for v1.0+
    from openai import OpenAI, AuthenticationError, RateLimitError, APIError, APIConnectionError, BadRequestError, APITimeoutError
except ImportError:
    print("ERROR: 'openai' library not found. Please install using: !pip install openai>=1.0")
    openai = None; OpenAI = None; AuthenticationError = None; RateLimitError = None; APIError = None; APIConnectionError = None; BadRequestError = None; APITimeoutError = None

try:
    import tiktoken
except ImportError:
    print("Warning: 'tiktoken' library not found. Install using: !pip install tiktoken")
    print("Token counting will use estimates.")
    tiktoken = None

# --- Colab Libraries ---
from google.colab import files # Import files for upload

# ==============================================================================
#                             CONFIGURATION BLOCK
# ==============================================================================
# (EDIT THESE VALUES)

# --- Input Data Sources ---
# Ground Truth dataset from Hugging Face
DATASET_NAME_HF = "Daoguang/Multi-SWE-bench"
DATASET_SPLIT_HF = "java_verified" # Adjust split as needed
# Locator file path - This will be set dynamically after upload below
# LOC_FILE_PATH = '/content/locator_output.json' # Example path

# --- Output Configuration ---
REPOS          = Path("/content/repos"); REPOS.mkdir(exist_ok=True) # Workspace dir for git checkouts
OUTPUT_FOLDER  = Path("/content/repair_output"); OUTPUT_FOLDER.mkdir(exist_ok=True) # Main output folder
PATCH_OUT      = OUTPUT_FOLDER / "java_patches_processed.jsonl" # FINAL processed output file path
RAW_OUTPUT_FILE = OUTPUT_FOLDER / "java_patches_raw.jsonl" # Intermediate raw LLM outputs
ARGS_FILE      = OUTPUT_FOLDER / "args_repair.json"        # To save config used
USED_LOCS_FILE = OUTPUT_FOLDER / "parsed_locations.jsonl" # To save parsed detailed_locations

# --- Model Configuration ---
OPENAI_MODEL   = "gpt-3.5-turbo" # Specify desired OpenAI model
# BACKEND is implicitly openai now
MAX_TOKENS     = 2048
TEMPERATURE    = 0.7   # Temperature for sampling (if NUM_SAMPLES > 1 and not SKIP_GREEDY)

# --- Generation Strategy ---
NUM_SAMPLES    = 3     # completions per instance (e.g., 1 greedy + N temperature)
SKIP_GREEDY    = False # Set to True to skip temperature=0 sample
USE_COT        = True  # Use Chain-of-Thought prompting
TOP_N          = 5     # Max number of unique files to consider from detailed_locations

# --- Context Construction ---
CONTEXT_LINES  = 10    # Lines of context around located snippets
# Use interval-based context fetching around specific locations
USE_LOC_INTERVAL = True
# Only use fine-grained context (methods/functions)? Set to False to allow wider context if specific lines aren't found
USE_FINE_GRAIN_LOC_ONLY = False
# ADD_SPACE_IN_CONTEXT = False # Less relevant for line-based context
# USE_STICKY_SCROLL = False # Not implemented

# --- Repair Format ---
EFFECTIVE_DIFF_FORMAT = True # Keep True for Java repair

# --- Execution Control ---
TARGET_INSTANCE_ID = None # Set to a specific ID to run only one
MOCK_API_CALLS = False # Set to True to simulate API calls

# --- Post-processing ---
POST_PROCESS_SELECT_ID = 0 # Index of the generation to process (-1 for all)


# ==============================================================================
#                            HELPER FUNCTIONS
# ==============================================================================

# --- File/Git Helpers ---
def load_any(fp):
    """Loads JSON array or JSONL file."""
    print(f"Loading data from: {fp}")
    if not Path(fp).exists(): print(f"Error: Input file not found at {fp}"); return None
    with open(fp) as f:
        import json
        try: data = json.load(f); print(f"Loaded as JSON object."); return [data] if isinstance(data, dict) else data
        except json.JSONDecodeError: f.seek(0); data = [json.loads(l) for l in f if l.strip()]; print(f"Loaded as JSONL ({len(data)} lines)."); return data
        except Exception as e: print(f"Error loading file {fp}: {e}"); return None

def clone_repo(repo: str, sha: str) -> Path | None:
    """Clone repository and checkout specific commit"""
    repo_name = repo.split('/')[-1]
    dst = REPOS / f"{repo_name}-{sha[:7]}"
    if not (dst/".git").exists():
        print(f"Cloning {repo} to {dst}...")
        try:
            subprocess.run(["git", "clone", "--quiet", f"https://github.com/{repo}.git", str(dst)], check=True, capture_output=True)
            print(f"Checking out {sha[:7]}...")
            subprocess.run(["git", "checkout", "--quiet", sha], cwd=str(dst), check=True, capture_output=True)
            print("Clone and checkout successful.")
        except subprocess.CalledProcessError as e: print(f"ERROR git op: {e.cmd}\n{e.stderr.decode(errors='ignore') if e.stderr else ''}"); return None
        except Exception as e: print(f"ERROR git op: {e}"); return None
    else: print(f"Repository already exists: {dst}")
    return dst

def get_file_content(repo_path: Path, file_path: str) -> str:
    """Get content of a file from the repository path"""
    full_path = repo_path / file_path
    content = ""
    if full_path.exists() and full_path.is_file():
        try: content = full_path.read_text(encoding='utf-8', errors='replace')
        except Exception as e: print(f"Warning: Could not read file {full_path}: {e}")
    # else: print(f"Warning: File not found at {full_path}") # Can be noisy
    return content

def run_git_command(command: list[str], working_dir: str, timeout: int = 60) -> tuple[str, str, int]:
     try:
          process = subprocess.run(command, cwd=working_dir, capture_output=True, text=True, encoding='utf-8', errors='replace', timeout=timeout, check=False)
          return process.stdout, process.stderr, process.returncode
     except FileNotFoundError: print("Error: 'git' command not found."); return "", "Git not found", -3
     except subprocess.TimeoutExpired: return "", "TimeoutExpired", -1
     except Exception as e: return "", str(e), -2

def fake_git_repo(repo_dir: Path, edited_files: list[str], original_contents: list[str], new_contents: list[str]) -> str:
    """Creates temp git repo to get diff."""
    if not edited_files: return ""
    if repo_dir.exists(): shutil.rmtree(repo_dir)
    repo_dir.mkdir(parents=True, exist_ok=True)
    _, stderr, ret = run_git_command(["git", "init"], str(repo_dir));
    if ret != 0: print(f"Error git init: {stderr}"); return ""
    run_git_command(["git", "config", "user.email", "a@c.com"], str(repo_dir)); run_git_command(["git", "config", "user.name", "AC"], str(repo_dir))
    for i, fp in enumerate(edited_files):
        full_p = repo_dir / fp; full_p.parent.mkdir(parents=True, exist_ok=True)
        try: full_p.write_text(original_contents[i], encoding='utf-8', errors='replace')
        except IOError as e: print(f"Error writing original {fp}: {e}")
    run_git_command(["git", "add", "."], str(repo_dir)); run_git_command(["git", "commit", "-m", "Original", "--allow-empty"], str(repo_dir))
    for i, fp in enumerate(edited_files):
        full_p = repo_dir / fp
        try: full_p.write_text(new_contents[i], encoding='utf-8', errors='replace')
        except IOError as e: print(f"Error writing new {fp}: {e}")
    stdout, stderr, ret = run_git_command(["git", "diff", "--no-prefix", "--no-color"], str(repo_dir))
    shutil.rmtree(repo_dir)
    if ret != 0: print(f"Error git diff: {stderr}"); return ""
    return stdout


# --- LLM and Tokenizer Helpers ---
def num_tokens_from_messages(messages, model=OPENAI_MODEL):
    """Returns the number of tokens used by a list of messages."""
    if tiktoken is None:
        total_chars = sum(len(str(v)) for message in messages for v in message.values()); return int(total_chars / 3.5) + len(messages) * 4
    try: encoding = tiktoken.encoding_for_model(model)
    except KeyError: encoding = tiktoken.get_encoding("cl100k_base")
    num_tokens = 0
    for message in messages:
        num_tokens += 3
        for key, value in message.items():
            try: num_tokens += len(encoding.encode(str(value)))
            except Exception: num_tokens += int(len(str(value)) / 3.5)
    num_tokens += 3
    return num_tokens

class OpenAIClient:
    """Handles OpenAI API calls."""
    def __init__(self, model, max_tokens, temperature):
        self.model, self.max_tokens, self.temperature = model, max_tokens, temperature
        self.client = None; self._configure_client()

    def _configure_client(self):
        """Configures the OpenAI client using a HARDCODED API key."""
        # Ensure the necessary libraries were imported successfully
        if openai is None or OpenAI is None:
            print("ERROR: OpenAI library components not available. Please check installation and imports.")
            return # Cannot configure client

        # --- Hardcode your key here ---
        # Replace "sk-proj-..." below with your actual key
        # WARNING: This is insecure! Your key is visible in the code.
        # Avoid sharing this notebook if you hardcode the key.
        hardcoded_api_key = "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" # PASTE YOUR KEY HERE

        # Basic check if the key looks somewhat valid
        if hardcoded_api_key and hardcoded_api_key.startswith("sk-"):
            try:
                # Directly use the hardcoded key to initialize the client
                self.client = OpenAI(api_key=hardcoded_api_key)
                print("✅ OpenAI client configured using HARDCODED value.")
            except Exception as e:
                # Catch potential errors during client instantiation
                print(f"🚨 Error initializing OpenAI client with hardcoded key: {e}")
                self.client = None # Ensure client is None if init fails
        else:
            print("❌ Error: The provided hardcoded API key is invalid or empty. Cannot configure client.")
            self.client = None

    def _make_api_call(self, messages, temp_override=None):
        if self.client is None: return {"response": "ERROR: Client not configured", "usage": {"prompt_tokens": 0, "completion_tokens": 0}}
        current_temp = temp_override if temp_override is not None else self.temperature
        print(f"--- Calling OpenAI: model={self.model}, temp={current_temp} ---")
        try:
            prompt_tokens = num_tokens_from_messages(messages, self.model); print(f"Est. prompt tokens: {prompt_tokens}")
            response = self.client.chat.completions.create(model=self.model, messages=messages, max_tokens=self.max_tokens, temperature=current_temp)
            generated_text = response.choices[0].message.content; usage_obj = response.usage
            usage_dict = {"prompt_tokens": usage_obj.prompt_tokens, "completion_tokens": usage_obj.completion_tokens, "total_tokens": usage_obj.total_tokens}
            print(f"API usage: {usage_dict}")
            return {"response": generated_text, "usage": usage_dict}
        except AuthenticationError as e: print(f"ERROR: OpenAI Auth Error: {e}"); p_tokens = num_tokens_from_messages(messages, self.model); return {"response": f"ERROR: Auth", "usage": {"prompt_tokens": p_tokens, "completion_tokens": 0}}
        except RateLimitError as e: print(f"ERROR: OpenAI Rate Limit Error: {e}"); p_tokens = num_tokens_from_messages(messages, self.model); return {"response": f"ERROR: Rate Limit", "usage": {"prompt_tokens": p_tokens, "completion_tokens": 0}}
        except APIConnectionError as e: print(f"ERROR: OpenAI Connection Error: {e}"); p_tokens = num_tokens_from_messages(messages, self.model); return {"response": f"ERROR: Connection", "usage": {"prompt_tokens": p_tokens, "completion_tokens": 0}}
        except BadRequestError as e: print(f"ERROR: OpenAI Bad Request Error: {e}"); p_tokens = num_tokens_from_messages(messages, self.model); return {"response": f"ERROR: Bad Request", "usage": {"prompt_tokens": p_tokens, "completion_tokens": 0}}
        except APITimeoutError as e: print(f"ERROR: OpenAI Timeout Error: {e}"); p_tokens = num_tokens_from_messages(messages, self.model); return {"response": f"ERROR: Timeout", "usage": {"prompt_tokens": p_tokens, "completion_tokens": 0}}
        except APIError as e: print(f"ERROR: OpenAI API Error Status={e.status_code} Msg={e.message}"); p_tokens = num_tokens_from_messages(messages, self.model); return {"response": f"ERROR: API Error", "usage": {"prompt_tokens": p_tokens, "completion_tokens": 0}}
        except Exception as e: print(f"ERROR: OpenAI General Exception: {e}"); p_tokens = num_tokens_from_messages(messages, self.model); return {"response": f"ERROR: General", "usage": {"prompt_tokens": p_tokens, "completion_tokens": 0}}

    def codegen(self, prompt, num_samples=1, prompt_cache=False):
        responses = []; messages = [{"role": "user", "content": prompt}]
        for i in range(num_samples):
            current_temp = 0.0 if i == 0 and not SKIP_GREEDY else self.temperature
            print(f"Generating sample {i + 1}/{num_samples} with effective temp={current_temp}")
            responses.append(self._make_api_call(messages, temp_override=current_temp))
        return responses

def make_llm_client(model, backend, max_tokens, temperature):
    if backend == 'openai': return OpenAIClient(model, max_tokens, temperature)
    else: print(f"ERROR: Backend '{backend}' not supported."); return None

# --- Text Processing and Patching Helpers ---
def extract_code_blocks(raw_output: str) -> list[str]:
    pattern = r"```(?:[a-zA-Z0-9]*)\s*\n(.*?)\n```"
    blocks = re.findall(pattern, raw_output, re.DOTALL)
    if not blocks: pattern_no_lang = r"```\s*\n(.*?)\n```"; blocks = re.findall(pattern_no_lang, raw_output, re.DOTALL)
    if not blocks and raw_output.strip().startswith("###") and "SEARCH" in raw_output: return [raw_output.strip()]
    elif not blocks: return [raw_output.strip()] # Fallback
    return [block.strip() for block in blocks]

def split_edit_multifile_commands(commands_str: str) -> dict[str, list]:
    """Splits SEARCH/REPLACE diff format only."""
    file_to_commands = defaultdict(list); current_file = None
    lines = commands_str.strip().split('\n'); current_block_lines = []
    for line in lines:
        if line.startswith("### "):
            if current_file and current_block_lines: file_to_commands[current_file].append("\n".join(current_block_lines).strip())
            current_file = line[4:].strip(); current_block_lines = [line]
        elif current_file: current_block_lines.append(line)
    if current_file and current_block_lines: file_to_commands[current_file].append("\n".join(current_block_lines).strip())
    return dict(file_to_commands)

def parse_diff_edit_commands(commands: list[str], original_content: str) -> str | None:
    """Applies a sequence of SEARCH/REPLACE blocks."""
    current_content = original_content; applied_count = 0
    for block_num, block_str in enumerate(commands):
        lines = block_str.strip().split('\n')
        if not lines or not lines[0].startswith("### "): print(f"Error: Invalid block header {block_num+1}. Skip."); continue
        try:
            s_idx, eq_idx, r_idx = lines.index("<<<<<<< SEARCH"), lines.index("======="), lines.index(">>>>>>> REPLACE")
            if not (0 < s_idx < eq_idx < r_idx): raise ValueError("Markers fail")
            search = "" if s_idx + 1 == eq_idx else "\n".join(lines[s_idx + 1 : eq_idx]) + "\n"
            replace = "" if eq_idx + 1 == r_idx else "\n".join(lines[eq_idx + 1 : r_idx]) + "\n"
        except ValueError as e: print(f"Error: Malformed block {block_num+1}: {e}. Skip."); continue
        if search == "": print(f"Applying ADD/REPLACE {block_num+1}"); current_content = replace; applied_count += 1
        else:
            found_idx = current_content.find(search)
            if found_idx != -1: print(f"Applying SEARCH/REPLACE {block_num+1}"); current_content = current_content[:found_idx] + replace + current_content[found_idx + len(search):]; applied_count += 1
            else: print(f"Error: SEARCH block {block_num+1} not found. Skip.")
    if applied_count == 0: return None
    if current_content == original_content: return None
    return current_content

def check_code_differ_by_just_empty_lines(new_contents: list[str], original_contents: list[str]) -> bool:
    if len(new_contents) != len(original_contents): return False
    for new_c, old_c in zip(new_contents, original_contents):
        new_lines = [l.strip() for l in new_c.splitlines() if l.strip()]; old_lines = [l.strip() for l in old_c.splitlines() if l.strip()]
        if new_lines != old_lines: return False
    return True

# --- Context Building Helpers (Restored/Adapted) ---
def parse_location_string(loc_detail: dict, file_lines: list[str]) -> tuple[int | None, int | None, str | None]:
    """Tries to parse detailed location into start/end lines and a string identifier."""
    line_num = loc_detail.get('line_number'); method_name = loc_detail.get('method_name'); class_name = loc_detail.get('class_name')
    loc_str = None
    if method_name and class_name and method_name != '<init>': loc_str = f"{class_name}.{method_name}"
    elif class_name: loc_str = class_name
    elif method_name: loc_str = method_name
    elif line_num is not None: loc_str = str(line_num)
    if isinstance(line_num, int) and line_num > 0: return line_num, line_num, loc_str
    if loc_str:
        try:
            for i, line in enumerate(file_lines):
                # Simple search refinement: check for class/method keywords too
                if loc_str in line and (f"class {loc_str}" in line or f"interface {loc_str}" in line or f"void {loc_str}(" in line or f"{loc_str}(" in line):
                     return i + 1, i + 1, loc_str
        except Exception: pass
    return None, None, loc_str # Return string even if line not found

def transfer_arb_locs_to_locs(
    loc_details: list[dict], # Changed from loc_strs
    structure: dict,         # No longer used if file_content provided
    file_path: str,
    context_window: int,
    loc_interval: bool,      # Flag to control interval expansion
    fine_grain_loc_only: bool, # Flag behaviour needs clarification
    file_content: str
) -> tuple[list[str], list[tuple[int, int]]]:
    """Converts detailed_locations list into context intervals and location strings."""
    line_intervals, context_intervals = [], []
    location_strings = []
    if not file_content: print(f"Warn: No content for {file_path}. Cannot get intervals."); return [], []
    file_lines = file_content.splitlines(); num_lines = len(file_lines)
    if num_lines == 0: return [], []

    # Parse detailed locations into line numbers and descriptive strings
    for loc_detail in loc_details:
        start, end, loc_str = parse_location_string(loc_detail, file_lines)
        if start is not None:
             start = max(1, min(start, num_lines)); end = max(start, min(end, num_lines))
             line_intervals.append((start, end))
        if loc_str: location_strings.append(loc_str)
    unique_location_strings = sorted(list(set(location_strings)))

    if not line_intervals:
        # If no specific line numbers found, should we use the whole file?
        # Option: If not fine_grain_loc_only, return interval for whole file?
        if not fine_grain_loc_only:
             print(f"Warn: No specific lines parsed for {file_path}. Using full file context (if not fine_grain_loc_only).")
             # Return interval covering whole file? Or let context builder handle?
             # Let's return an empty context interval list here, context builder can decide.
             return unique_location_strings, []
        else:
            print(f"Info: No specific lines parsed for {file_path} and fine_grain_loc_only=True. No context intervals.")
            return unique_location_strings, []

    # Merge overlapping/adjacent intervals derived from line numbers
    line_intervals.sort(); merged_intervals = []
    cs, ce = line_intervals[0]
    for ns, ne in line_intervals[1:]:
        if ns <= ce + 1: ce = max(ce, ne)
        else: merged_intervals.append((cs, ce)); cs, ce = ns, ne
    merged_intervals.append((cs, ce))

    # Expand intervals with context window if loc_interval is True
    if loc_interval:
        for start, end in merged_intervals:
            context_intervals.append((max(1, start - context_window), min(num_lines, end + context_window)))
    else: # If not using intervals, just use the merged blocks directly?
        context_intervals = merged_intervals # Use original merged blocks without window

    if not context_intervals: return unique_location_strings, []

    # Merge context intervals again after potential expansion
    context_intervals.sort(); final_merged_context = []
    ccs, cce = context_intervals[0]
    for ncs, nce in context_intervals[1:]:
        if ncs <= cce + 1: cce = max(cce, nce)
        else: final_merged_context.append((ccs, cce)); ccs, cce = ncs, nce
    final_merged_context.append((ccs, cce))

    return unique_location_strings, final_merged_context

def line_wrap_content(content: str, intervals: list[tuple[int, int]], no_line_number: bool = False) -> str:
    """Extracts and formats lines from content based on intervals, adding line numbers."""
    lines = content.splitlines(); num_lines = len(lines); output_lines = []
    last_line_num = 0; intervals.sort()
    for start, end in intervals:
        start = max(1, min(start, num_lines)); end = max(start, min(end, num_lines))
        if last_line_num > 0 and start > last_line_num + 1: output_lines.append("...") # Separator
        for i in range(start, end + 1):
             line_num_str = "" if no_line_number else f"{i}: " # Add line number prefix
             try: output_lines.append(f"{line_num_str}{lines[i-1]}")
             except IndexError: pass
        last_line_num = end
    return "\n".join(output_lines)


# --- Core Repair Logic Functions ---

def _post_process_multifile_repair(raw_output: str, file_contents: dict[str, str]) -> tuple[list[str], list[str]]:
    """Parses diff/replace format and applies edits. Assumes diff format."""
    edited_files, new_contents = [], []
    code_blocks = extract_code_blocks(raw_output)
    if not code_blocks: print("Error: No code blocks extracted."); return [], []
    commands_str = code_blocks[0]
    try: file_to_commands = split_edit_multifile_commands(commands_str)
    except Exception as e: print(f"Error splitting commands: {e}"); return [], []
    successfully_edited_files = {}
    for edited_file, commands in file_to_commands.items():
        print(f"-- Processing file edit: {edited_file} --")
        if edited_file not in file_contents: print(f"Error: File '{edited_file}' not in input."); continue
        content = file_contents[edited_file]
        try:
            new_content_for_file = parse_diff_edit_commands(commands, content)
            if new_content_for_file is not None: successfully_edited_files[edited_file] = new_content_for_file
            else: print(f"Applying commands failed for {edited_file}.")
        except Exception as e: print(f"Error processing commands for {edited_file}: {e}")
    edited_files = list(successfully_edited_files.keys()); new_contents = list(successfully_edited_files.values())
    if not edited_files: print("Warn: No files successfully edited.")
    return edited_files, new_contents

def construct_topn_file_context(
    loc_details_by_file: Dict[str, List[Dict]], # Map: file_path -> list of detailed loc dicts
    pred_files: List[str],
    file_contents: Dict[str, str],
    structure: Dict, # Dummy structure dict from get_repo_structure might be needed
    config: Dict
) -> Tuple[str, Dict]:
    """Builds the context string using detailed locations and context intervals."""
    file_loc_intervals_used = {}; topn_content = ""
    file_to_loc_strings = defaultdict(list) # Store parsed strings

    print(f"Building context from top {len(pred_files)} files using detailed locations...")

    for pred_file in pred_files:
        if pred_file not in file_contents:
             print(f"Warn: Content missing for {pred_file} during context build.")
             continue

        loc_details_for_file = loc_details_by_file.get(pred_file, [])
        content = file_contents[pred_file]

        # Get location strings and context intervals using the restored function
        loc_strings, context_intervals = transfer_arb_locs_to_locs(
            loc_details_for_file,
            structure, # Pass structure (may be dummy)
            pred_file,
            config['context_lines'],
            config['loc_interval'],
            config['fine_grain_loc_only'],
            content # Pass file content
        )
        file_to_loc_strings[pred_file] = loc_strings # Store parsed strings

        if context_intervals:
            # Use line_wrap_content (restored) to format with line numbers if needed
            no_lines = config['diff_format'] # Only omit line numbers if using diff format? Typically yes.
            file_loc_content = line_wrap_content(content, context_intervals, no_lines)
            topn_content += f"### {pred_file}\n```{LANGUAGE}\n{file_loc_content}\n```\n\n\n"
            file_loc_intervals_used[pred_file] = context_intervals
        else:
            print(f"No context intervals generated for {pred_file}.")
            # If no intervals, skip file? Or add truncated full content?
            # Let's skip for now if no intervals derived from locations.

    return topn_content, file_loc_intervals_used # Return intervals too


def process_loc(loc_entry: dict, swe_bench_map: dict, config: dict, raw_output_file_path: Path):
    """Generates repair for one instance using GT from dataset and loc from file."""
    instance_id = loc_entry.get("instance_id")
    if not instance_id: print("Error: Locator entry missing instance_id."); return
    if config['target_id'] is not None and config['target_id'] != instance_id: return

    print(f"\n================ Processing Instance: {instance_id} ================")

    # --- Get Ground Truth Data ---
    bench_data = swe_bench_map.get(instance_id)
    if not bench_data: print(f"Error: Ground truth data not found for {instance_id}. Skip."); return
    problem_statement = bench_data.get("problem_statement", ""); repo_slug = bench_data.get("repository", "") or bench_data.get("repo", ""); base_commit = bench_data.get("base_commit", "")
    if not all([problem_statement, repo_slug, base_commit]): print(f"Error: Missing essential GT fields for {instance_id}. Skip."); return

    # --- Get Localization Data ---
    detailed_locations = loc_entry.get("detailed_locations", [])
    if not detailed_locations: print(f"Warn: No detailed_locations found for {instance_id}. Using empty list."); detailed_locations = [] # Proceed, but context might be empty

    # --- Parse Detailed Locations ---
    locs_by_file = defaultdict(list)
    for loc_detail in detailed_locations:
        file_path = loc_detail.get("file_path")
        if file_path: locs_by_file[file_path].append(loc_detail)
    unique_files = list(locs_by_file.keys())
    if not unique_files: print(f"Warn: No file paths parsed from detailed_locations for {instance_id}."); # Allow proceeding, context will be empty

    pred_files_all = unique_files
    pred_files = pred_files_all[:config['top_n']] # Apply TOP_N limit
    print(f"Using top {len(pred_files)} of {len(pred_files_all)} unique files from locator: {pred_files}")

    # Save parsed locations (optional)
    parsed_locs_entry = {"instance_id": instance_id, "top_n_files": pred_files, "locations_by_file": dict(locs_by_file)}
    try:
        with open(config['used_locs_file'], "a") as f: f.write(json.dumps(parsed_locs_entry) + "\n")
    except IOError as e: print(f"Warn: Could not write parsed locations file: {e}")

    # --- Clone Repo ---
    print(f"Cloning repository {repo_slug} @ {base_commit}...")
    # Use the user's clone_repo function
    repo_path = clone_repo(repo_slug, base_commit)
    if not repo_path:
        print(f"Error: Failed to clone repository for {instance_id}. Skipping."); return

    # --- Get Content for Selected Files ---
    # Use the user's get_file_content function
    file_contents = {f: get_file_content(repo_path, f) for f in pred_files}
    # Filter out files where content reading failed or returned empty
    original_count = len(file_contents)
    file_contents = {f: c for f, c in file_contents.items() if c}
    if len(file_contents) < original_count:
         print(f"Warn: Could not read content for {original_count - len(file_contents)} files.")
    if not file_contents:
        print(f"Error: Could not read content for any of the top {len(pred_files)} files. Skipping."); return
    pred_files = list(file_contents.keys()) # Update pred_files based on successful reads

    # --- Construct Context (Using Detailed Locations) ---
    detailed_locs_for_context = {fp: locs_by_file[fp] for fp in pred_files}
    # Create a minimal dummy structure needed by transfer_arb_locs_to_locs (if any)
    # Or adapt transfer_arb_locs_to_locs not to need it
    dummy_structure = {'files': file_contents} # Pass actual content if needed by parser
    topn_content, file_loc_intervals = construct_topn_file_context(detailed_locs_for_context, pred_files, file_contents, dummy_structure, config)

    if not topn_content.strip(): print(f"Warn: No context constructed for {instance_id}. Skip."); return

    # --- Prompt ---
    prompt_template = repair_prompt_combine_topn_cot_diff
    message = prompt_template.format(repair_relevant_file_instruction=repair_relevant_file_instruction, problem_statement=problem_statement, content=topn_content.rstrip(), language="diff", example=DIFF_EXAMPLE).strip()
    print(f"Prompting model {config['model']}...")

    # --- Model Interaction ---
    all_generations, counts, traj, prev_contents, file_names, raw_outputs = [], [], [], [], [], []
    sample_responses = []
    llm_client = make_llm_client(config['model'], config['backend'], config['max_tokens'], config['temperature'])
    if llm_client is None: print("ERROR: Failed to create LLM client."); return

    if not config['skip_greedy']:
        if config['mock_api_calls']: sample_responses.append({"response": "", "usage": {"prompt_tokens": num_tokens_from_messages([{"role":"user", "content":message}], config['model'])}})
        else:
            try: sample_responses.extend(llm_client.codegen(message, 1))
            except Exception as e: print(f"Greedy gen failed: {e}")
    num_temp_samples = config['num_samples'] - len(sample_responses)
    if num_temp_samples > 0:
        if config['mock_api_calls']: sample_responses.extend([{"response": "", "usage": {"prompt_tokens": 0}}] * num_temp_samples)
        else:
             try: sample_responses.extend(llm_client.codegen(message, num_temp_samples))
             except Exception as e: print(f"Temp sampling failed: {e}")

    # --- Process Samples ---
    print(f"Processing {len(sample_responses)} samples...")
    count = 0
    for ret in sample_responses:
        count += 1; print(f"-- Sample {count} --")
        # Ensure usage is serializable dict
        usage_info = ret.get('usage', {})
        if hasattr(usage_info, 'model_dump'): usage_info = usage_info.model_dump()
        elif hasattr(usage_info, 'dict'): usage_info = usage_info.dict()
        elif isinstance(usage_info, object) and not isinstance(usage_info, dict): usage_info = vars(usage_info)
        ret['usage'] = usage_info
        traj.append({**ret, "prompt": "omitted"})

        if config['mock_api_calls']: raw_outputs.append(""); all_generations.append(""); prev_contents.append([]); file_names.append([]); counts.append(count); continue

        raw_output = ret.get("response", ""); raw_outputs.append(raw_output); all_generations.append(raw_output)
        if not raw_output or "ERROR:" in raw_output: # Skip processing if API call failed
            print("Skipping processing due to empty or error response.")
            prev_contents.append([]); file_names.append([]); counts.append(count)
            continue

        # Pass intervals used for context if needed by post-processing (unlikely now)
        edited, new_c = _post_process_multifile_repair(raw_output, file_contents)
        if not edited: prev_contents.append([]); file_names.append([])
        else: prev_contents.append([file_contents[f] for f in edited]); file_names.append(edited)
        counts.append(count)

    # --- Write Raw Output ---
    output_entry = {"instance_id": instance_id, "raw_output": raw_outputs, "all_generations": [all_generations], "try_count": counts, "traj": traj, "prev_content": [prev_contents], "file_names": [file_names], "status": "COMPLETED" if count > 0 else "FAILED"}
    try:
        with open(raw_output_file_path, "a") as f: f.write(json.dumps(output_entry) + "\n")
    except IOError as e: print(f"Error writing raw output file {raw_output_file_path}: {e}")

    # --- Process Samples ---
    print(f"Processing {len(sample_responses)} samples...")
    count = 0
    for ret in sample_responses:
        count += 1; print(f"-- Sample {count} --")
        traj.append({**ret, "prompt": "omitted"})
        if config['mock_api_calls']: raw_outputs.append(""); all_generations.append(""); prev_contents.append([]); file_names.append([]); counts.append(count); continue
        raw_output = ret.get("response", ""); raw_outputs.append(raw_output); all_generations.append(raw_output)
        # Pass intervals used for context if needed by post-processing (unlikely now)
        edited, new_c = _post_process_multifile_repair(raw_output, file_contents)
        if not edited: prev_contents.append([]); file_names.append([])
        else: prev_contents.append([file_contents[f] for f in edited]); file_names.append(edited)
        counts.append(count)

    # --- Write Raw Output ---
    output_entry = {"instance_id": instance_id, "raw_output": raw_outputs, "all_generations": [all_generations], "try_count": counts, "traj": traj, "prev_content": [prev_contents], "file_names": [file_names], "status": "COMPLETED" if count > 0 else "FAILED"}
    try:
        with open(raw_output_file_path, "a") as f: f.write(json.dumps(output_entry) + "\n")
    except IOError as e: print(f"Error writing raw output file {raw_output_file_path}: {e}")


def repair(config: dict, loc_file_path: str, swe_bench_map: dict):
    """Main function orchestrating repair generation."""
    try: config['args_file'].write_text(json.dumps(config, indent=4, default=str))
    except Exception as e: print(f"Warn: Could not write args file {config['args_file']}: {e}")

    locs = load_any(loc_file_path) # Load locator data
    if not locs: print("Error: Failed to load locator data."); return
    print(f"Loaded {len(locs)} localization results.")

    raw_output_file = config['raw_output_file']
    if raw_output_file.exists(): print(f"Clearing existing raw output file: {raw_output_file}"); raw_output_file.unlink()
    used_locs_path = config['used_locs_file']
    if used_locs_path.exists(): used_locs_path.unlink()
    print(f"Will write parsed locations to {used_locs_path}")

    instance_data_list_all = locs # Iterate through locator entries

    if config['target_id']:
        instance_data_list = [d for d in instance_data_list_all if d.get("instance_id") == config['target_id']]
        # ... (error check) ...
    else: # Limit to first 5 if no target ID specified
        instance_data_list = instance_data_list_all[:5] # <<< SLICING ADDED
        print(f"Processing first {len(instance_data_list)} instances...")


    print("Running repair generation sequentially...")
    for loc_entry in tqdm(locs, desc="Processing Instances"):
        # Pass locator entry, GT map, config, and raw output path
        process_loc(loc_entry, swe_bench_map, config, raw_output_file)

    print(f"Repair generation process finished. Raw outputs in {config['raw_output_file']}")


def post_process_raw_output(raw_output_text: str, file_contents: dict, config: dict) -> tuple:
    """Applies post-processing, generates git diff."""
    git_diffs, raw_git_diffs, original_contents, edited_files, new_contents = "", "", [], [], []
    try:
        # Pass dummy intervals, post_process only needs raw_output and file_contents map now
        edited_files, new_contents = _post_process_multifile_repair(raw_output_text, file_contents)
        if not edited_files: print("Warn: Post-processing yielded no edits."); return "", "", [], [], []
        original_contents = [file_contents[f] for f in edited_files]
        temp_repo_dir = Path(config['output_folder']) / "temp_git_repo"
        git_diff = fake_git_repo(temp_repo_dir, edited_files, original_contents, new_contents)
        raw_git_diffs = git_diff
        differ_by_empty = check_code_differ_by_just_empty_lines(new_contents, original_contents)
        patch_applied = bool(git_diff.strip())
        if patch_applied and not differ_by_empty: git_diffs = raw_git_diffs; print("Diff valid.")
        else: git_diffs = ""; print("Warn: Diff discarded.")
    except Exception as e: print(f"Error during post_process_raw_output: {e}"); return "", "", [], [], []
    return git_diffs, raw_git_diffs, original_contents, edited_files, new_contents

def post_process_repair(config: dict):
    """Reads raw outputs and applies post-processing."""
    raw_output_file = config['raw_output_file']
    processed_output_file = config['patch_out']
    print(f"Starting post-processing for {raw_output_file} -> {processed_output_file}")
    if not raw_output_file.exists(): print(f"Error: Raw output file not found: {raw_output_file}"); return

    raw_outputs_data = load_any(raw_output_file)
    if not raw_outputs_data: print("No data in raw output file."); return

    processed_results = []
    for raw_output_entry in tqdm(raw_outputs_data, desc="Post-processing"):
        instance_id = raw_output_entry["instance_id"]
        print(f"-- Post-processing instance {instance_id} --")
        if raw_output_entry.get("status") != "COMPLETED": print(f"Skipping {instance_id}, status: {raw_output_entry.get('status')}"); continue

        generations_to_process = []
        try: # Extract generations data carefully
             all_gens, all_prev, all_fnames = raw_output_entry.get('all_generations', [[]])[0], raw_output_entry.get('prev_content', [[]])[0], raw_output_entry.get('file_names', [[]])[0]
             num_gens = len(all_gens); idx = config['select_id']
             if idx == -1: generations_to_process = [(i, all_gens[i], all_prev[i], all_fnames[i]) for i in range(num_gens)] if len(all_prev)==num_gens and len(all_fnames)==num_gens else []
             elif 0 <= idx < num_gens and len(all_prev) > idx and len(all_fnames) > idx: generations_to_process.append((idx, all_gens[idx], all_prev[idx], all_fnames[idx]))
             if not generations_to_process: print(f"Error selecting generation {idx} for {instance_id}."); continue
        except Exception as e: print(f"Error accessing generation data for {instance_id}: {e}"); continue

        for gen_idx, raw_txt, orig_cont_list, edit_files_list in generations_to_process:
             print(f"- Applying post-processing to generation {gen_idx} -")
             if not raw_txt or not edit_files_list or not orig_cont_list: print("Skipping: Empty data."); continue
             file_cont_dict = dict(zip(edit_files_list, orig_cont_list))
             # Pass dummy intervals
             git_diffs, raw_git, proc_orig, proc_edit, proc_new = post_process_raw_output(raw_txt, file_cont_dict, config) # Removed intervals arg
             processed_results.append({
                 "model_name_or_path": config['model'], "instance_id": instance_id, "generation_index": gen_idx,
                 "model_patch": git_diffs.strip(), "raw_model_patch": raw_git.strip(),
                 "original_file_content": proc_orig, "edited_files": proc_edit, "new_file_content": proc_new,
             })

    print(f"Writing {len(processed_results)} processed results to {processed_output_file}")
    try:
        with open(processed_output_file, "w") as f: [f.write(json.dumps(res) + "\n") for res in processed_results]
        print("Post-processing finished successfully.")
    except IOError as e: print(f"Error writing processed output file {processed_output_file}: {e}")

# --- Prompts ---
repair_relevant_file_instruction = "Relevant code context:"
repair_prompt_combine_topn_cot_diff = """Resolve the following issue given the context.
ISSUE:
{problem_statement}
---
{repair_relevant_file_instruction}
{content}
---
Generate **one or more** `SEARCH/REPLACE` blocks to fix the issue. Format ONLY with SEARCH/REPLACE blocks wrapped in a single ```diff ... ``` block.

Example:
{example}
```diff
### path/to/file.java
<<<<<<< SEARCH
Existing code lines to replace
=======
New code lines
>>>>>>> REPLACE
```"""
LANGUAGE = "java"
DIFF_EXAMPLE = """
```diff
--- a/src/main/java/com/example/MyClass.java
+++ b/src/main/java/com/example/MyClass.java
@@ -10,7 +10,7 @@
     * Some javadoc.
     */
    public void myMethod() {
-        System.out.println("Old line");
+        System.out.println("New fixed line");
         int x = 5;
         // another comment
    }
```
"""



def run_repair_colab(loc_file_path: str, dataset_name: str, dataset_split: str):
    """Main function: Loads GT from HF, Loc from file, runs repair & post-process."""

    # --- Load Ground Truth Data ---
    swe_bench_data = None
    print(f"Loading ground truth dataset: {dataset_name} split {dataset_split}")
    if load_dataset is None: print("ERROR: datasets library not loaded."); return
    try:
        swe_bench_data_list = list(load_dataset(dataset_name, split=dataset_split, trust_remote_code=True))
        swe_bench_map = {item['instance_id']: item for item in swe_bench_data_list}
        print(f"Loaded and mapped {len(swe_bench_map)} instances from ground truth dataset.")
    except Exception as e:
        print(f"ERROR: Failed to load or map ground truth dataset '{dataset_name}': {e}"); return

    # --- Consolidate config from globals ---
    config = {k: v for k, v in globals().items() if k.isupper() and not k.startswith('_') and k != 'REPOS'}
    config['output_folder'] = str(OUTPUT_FOLDER); config['workspace_dir'] = str(REPOS)
    config['repos'] = REPOS; config['patch_out'] = PATCH_OUT; config['raw_output_file'] = RAW_OUTPUT_FILE
    config['args_file'] = ARGS_FILE; config['used_locs_file'] = USED_LOCS_FILE
    config['diff_format'] = EFFECTIVE_DIFF_FORMAT; config['str_replace_format'] = False
    config['model'] = config.pop('OPENAI_MODEL')
    config['target_id'] = config.pop('TARGET_INSTANCE_ID', None)
    config['top_n'] = config.pop('TOP_N', 3)
    config['backend'] = 'openai' # Hardcode backend based on client used
    config['num_samples'] = config.pop('NUM_SAMPLES', 3)
    config['context_lines'] = config.pop('CONTEXT_LINES', 10)
    config['max_tokens'] = config.pop('MAX_TOKENS', 2048)
    config['temperature'] = config.pop('TEMPERATURE', 0.7)
    config['skip_greedy'] = config.pop('SKIP_GREEDY', False)
    config['use_cot'] = config.pop('USE_COT', True)
    config['loc_interval'] = config.pop('USE_LOC_INTERVAL', True)
    config['fine_grain_loc_only'] = config.pop('USE_FINE_GRAIN_LOC_ONLY', False)
    # Add 'add_space' and 'sticky_scroll' if they are used by context building
    config['add_space'] = config.pop('ADD_SPACE_IN_CONTEXT', False) # Check global name
    config['sticky_scroll'] = config.pop('USE_STICKY_SCROLL', False) # Check global name
    config['mock_api_calls'] = config.pop('MOCK_API_CALLS', False)
    config['select_id'] = config.pop('POST_PROCESS_SELECT_ID', 0)


    Path(config['output_folder']).mkdir(exist_ok=True)
    Path(config['output_folder'], "repair_logs").mkdir(exist_ok=True) # For placeholders

    # --- Run Repair ---
    print("--- Running Generation Phase ---")
    repair(config, loc_file_path, swe_bench_map) # Pass GT map and locator file path
    print("--- Generation Phase Complete ---")

    # --- Run Post-Processing ---
    print("\n--- Running Post-Processing Phase ---")
    if config['raw_output_file'].exists():
        post_process_repair(config) # Post-process only needs config now
    else:
        print(f"Raw output file ({config['raw_output_file']}) not found. Skipping post-processing.")
    print("--- Post-Processing Phase Complete ---")

#Cell 1: Install Dependencies (Run once per session)
!pip install datasets tqdm openai tiktoken # Add others if needed
!apt-get update && apt-get install -y git

#Cell 2: Upload Locator File
print("⬆️ Upload the locator JSON/JSONL file (containing detailed_locations)")
loc_upload = files.upload()
LOC_FILE_PATH = next(iter(loc_upload))
print(f"Locator file set to: {LOC_FILE_PATH}")

#Cell 5: Run the Repair Process
# Ensure LOC_FILE_PATH is set from Cell 2 upload.
# Ensure API keys are configured and placeholders are implemented.
if 'LOC_FILE_PATH' in locals() and Path(LOC_FILE_PATH).exists():
  print("Starting repair process...")
  # Set configuration block at the top of the script correctly!
  run_repair_colab(
    loc_file_path=LOC_FILE_PATH,
    dataset_name=DATASET_NAME_HF,
    dataset_split=DATASET_SPLIT_HF
  )
  print(f"\nProcessing complete.")
  print(f"Raw output written to: {RAW_OUTPUT_FILE}")
  print(f"Final processed patches written to: {PATCH_OUT}")
else:
  print("Please run the locator file upload cell (Cell 2) first and ensure LOC_FILE_PATH points to an existing file.")
