# -*- coding: utf-8 -*-
"""AgentlessStep3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18v6jFbUaKCCjVcTkvM_yskedcxORdWxl
"""

!pip install llama-index-embeddings-google-genai datasets numpy tqdm

!pip install tree-sitter-java
!pip install tree-sitter

import tree_sitter_java
from tree_sitter import Language, Parser
import hashlib

#!/usr/bin/env python3
"""
suspicious_files_locator.py - Identifies suspicious files in Java repositories using embedding-based retrieval

This script processes the Multi-SWE-bench Java dataset and identifies suspicious files that are
likely relevant to bug issues using embedding-based retrieval with Google's Gemini embedding model.

Usage:
    python suspicious_files_locator.py --output suspicious_files.json
                                      [--api_key YOUR_GOOGLE_API_KEY]
                                      [--top_n 20]
"""

import argparse
import json
import os
import logging
import re
import subprocess
import sys
from typing import Dict, List, Any
import numpy as np
from tqdm import tqdm
from datasets import load_dataset

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding


def clone_repo(repo_url, base_commit, clone_dir):
    """
    Clones a repository at a specific commit.

    Args:
        repo_url: Repository URL in format 'org/repo'
        base_commit: Git commit hash to checkout
        clone_dir: Directory to clone into

    Returns:
        Path to the cloned repository
    """
    if not os.path.exists(clone_dir):
        os.makedirs(clone_dir)

    repo_name = repo_url.split("/")[-1]  # Extract repo name
    repo_path = os.path.join(clone_dir, repo_name)

    if os.path.exists(repo_path):
        logger.info(f"Repository {repo_name} already exists, skipping clone...")
    else:
        logger.info(f"Cloning {repo_url} at commit {base_commit}...")
        try:
            result = subprocess.run(
                ["git", "clone", f"https://github.com/{repo_url}.git", repo_path],
                capture_output=True,
                text=True,
                check=True
            )
        except subprocess.CalledProcessError as e:
            logger.error(f"Error cloning repository: {e}")
            logger.error(f"stdout: {e.stdout}")
            logger.error(f"stderr: {e.stderr}")
            return None

    # Checkout the base commit to match dataset state
    try:
        # Clean untracked files before checkout to avoid conflicts
        subprocess.run(
            ["git", "-C", repo_path, "clean", "-fd"],
            check=True
        )

        # Reset any changes
        subprocess.run(
            ["git", "-C", repo_path, "reset", "--hard"],
            check=True
        )

        # Checkout the base commit
        subprocess.run(
            ["git", "-C", repo_path, "checkout", base_commit],
            check=True
        )
    except subprocess.CalledProcessError as e:
        logger.error(f"Error checking out commit {base_commit}: {e}")
        return None

    return repo_path



def find_java_files(repo_path: str, tree: list) -> Dict[str, str]:
    """
    Find all Java files in a repository based on the provided tree structure and read their content

    Args:
        repo_path: The path to the cloned repository
        tree: A list of strings representing a directory tree structure

    Returns:
        Dictionary mapping file paths to their content
    """
    java_files = {}

    # Parse the tree to build a proper directory structure
    # Each level will be represented as a list of [parent_index, name]
    entries = []

    for line in tree:
        # Skip empty lines
        if not line.strip():
            continue

        # Extract the name by removing all tree symbols and spaces
        name = re.sub(r'[\u251c\u2514\u2502\u2500\s]+', '', line).strip()

        # Skip lines with no name
        if not name:
            continue

        # Calculate position of the actual name in the string to determine depth
        name_start = line.find(name.split('/')[0] if '/' in name else name)
        if name_start < 0:
            name_start = 0

        # Determine depth by the position of the name
        depth = name_start // 4

        # Find parent based on depth
        parent_index = -1
        for i in range(len(entries) - 1, -1, -1):
            if entries[i][0] < depth:
                parent_index = i
                break

        entries.append([depth, name, parent_index])

    # Now build file paths and read Java files
    for i, (depth, name, parent_index) in enumerate(entries):
        if name.endswith(".java"):
            # Build the path by traversing up the parent chain
            path_parts = [name]
            current_index = parent_index

            while current_index >= 0:
                parent_depth, parent_name, grandparent_index = entries[current_index]
                # Remove trailing slash if present
                if parent_name.endswith("/"):
                    parent_name = parent_name[:-1]
                path_parts.insert(0, parent_name)
                current_index = grandparent_index

            file_path = "/".join(path_parts)

            try:
                full_path = os.path.join(repo_path, file_path)
                with open(full_path, 'r', encoding='utf-8') as f:
                    full_text = f.read()
                java_files[file_path] = text2strskeleton(full_text)
            except Exception as e:
                logger.warning(f"Could not read file {file_path}: {e}")

    return java_files

class FileRetriever:
    """Retrieve relevant files using embedding-based similarity, now parsing Java code with Tree-Sitter."""

    def __init__(self, embedding_model, chunk_size=1024, chunk_overlap=128):
        """
        :param embedding_model: Your embedding model object, e.g. GoogleGenAIEmbedding or OpenAIEmbedding
        :param tree_sitter_java: A Language object compiled from tree-sitter-java grammar, e.g.:
                                Language('tree-sitter-java.so', 'java')
        :param chunk_size, chunk_overlap: If you still want text-based chunking for fallback or extremely large methods.
        """
        self.embedding_model = embedding_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # Cache for chunk embeddings: { chunk_hash: embedding_vector }
        self.chunk_cache = {}

        # Tree-Sitter parser for Java
        JAVA_LANGUAGE = Language(tree_sitter_java.language())
        self.parser = Parser(JAVA_LANGUAGE)

    def _extract_java_entities(self, code: str) -> List[str]:
        """
        Parse a Java file with Tree-Sitter and return code chunks for:
          - class_declaration or interface_declaration blocks
          - method_declaration inside a class/interface
        You can expand to handle other node types if desired, e.g. constructor_declaration.
        """
        tree = self.parser.parse(bytes(code, "utf-8"))
        root_node = tree.root_node

        extracted_segments = []

        # Helper function to slice code text from node positions
        def get_code_fragment(node):
            return code[node.start_byte : node.end_byte]

        # Recursively traverse the AST
        def walk(node, parent_is_class=False):
            # If this is a class or interface, record the entire chunk
            if node.type in ("class_declaration", "interface_declaration"):
                extracted_segments.append(get_code_fragment(node))
                # Then walk children, marking them as within a class
                for child in node.children:
                    walk(child, parent_is_class=True)
            elif parent_is_class and node.type == "method_declaration":
                # This is a method inside a class (or interface)
                extracted_segments.append(get_code_fragment(node))
            else:
                # For everything else, just recurse
                for child in node.children:
                    walk(child, parent_is_class=parent_is_class)

        walk(root_node)

        # If no structural chunks found (e.g. parse errors), you can fallback to the entire file
        if not extracted_segments:
            return [code]

        return extracted_segments

    def _cosine_similarity(self, v1: List[float], v2: List[float]) -> float:
        if not v1 or not v2:
            return 0.0
        dot_product = np.dot(np.array(v1), np.array(v2))
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot_product / (norm1 * norm2)
    def retrieve_suspicious_files(
        self, issue_description: str, java_files: Dict[str, str], top_n: int = 20
    ) -> List[str]:
        """
        Retrieve suspicious Java files based on embedding similarity,
        now using a single batch embedding pass for all unique code chunks.
        """
        # 1) Get the embedding for the issue description
        issue_embedding = self.embedding_model.get_text_embedding(issue_description)

        # 2) Gather all unique chunks (across all files) that aren't in cache
        #    We'll store a tuple (chunk_text, chunk_hash) for each
        to_embed = []  # e.g. list of (chunk_text, chunk_hash)
        # Also keep track of which file(s) each chunk belongs to
        file_to_chunks = {}  # file_path -> list of chunk_hash
        for file_path, content in java_files.items():
            code_segments = self._extract_java_entities(content)
            file_to_chunks[file_path] = []
            for seg in code_segments:
                c_hash = hashlib.md5(seg.encode("utf-8")).hexdigest()
                file_to_chunks[file_path].append(c_hash)
                if c_hash not in self.chunk_cache:  # not embedded yet
                    to_embed.append((seg, c_hash))

        # 3) Embed all new chunks in batches
        BATCH_SIZE = 150  # pick your batch size
        print(len(to_embed))
        for i in range(0, len(to_embed), BATCH_SIZE):
            batch = to_embed[i : i + BATCH_SIZE]
            texts_for_batch = [x[0] for x in batch]
            hashes_for_batch = [x[1] for x in batch]

            # call your embedding_model.get_text_embedding_batch(...)
            batch_embeddings = self.embedding_model.get_text_embedding_batch(texts_for_batch)
            # store in cache
            for j, emb in enumerate(batch_embeddings):
                self.chunk_cache[hashes_for_batch[j]] = emb

        # 4) Now compute file-level similarity with the embeddings
        file_scores = {}
        for file_path, chunk_hashes in file_to_chunks.items():
            file_max_sim = 0.0
            for c_hash in chunk_hashes:
                emb = self.chunk_cache.get(c_hash, [])
                if emb:
                    sim = self._cosine_similarity(issue_embedding, emb)
                    if sim > file_max_sim:
                        file_max_sim = sim
            file_scores[file_path] = file_max_sim

        # 5) Sort files by descending similarity, return top N
        ranked_files = sorted(file_scores.items(), key=lambda x: x[1], reverse=True)
        return [fp for fp, _ in ranked_files[:top_n]]



def process_instance(instance_data: Dict, tree: list, embedding_model, top_n: int = 20, clone_dir: str = "temp_repos") -> Dict:
    """
    Process a single instance to identify suspicious files

    Args:
        instance_data: Dictionary containing instance information
        embedding_model: Embedding model for similarity calculation
        top_n: Number of top suspicious files to return
        clone_dir: Directory to clone repositories into

    Returns:
        Dictionary with instance information and suspicious files
    """
    repo = instance_data["repo"]
    instance_id = instance_data["instance_id"]
    problem_statement = instance_data["problem_statement"]
    base_commit = instance_data["base_commit"]

    logger.info(f"Processing instance {instance_id} from repository {repo}")

    # Clone repository at the specific commit
    repo_path = clone_repo(repo, base_commit, clone_dir)
    print(repo_path)
    if not repo_path:
        logger.error(f"Failed to clone repository {repo}")
        return {
            "repo": repo,
            "instance_id": instance_id,
            "suspicious_files": []
        }

    # Find all Java files in the repository
    java_files = find_java_files(repo_path, tree)
    logger.info(f"Found {len(java_files)} Java files in repository {repo}")

    if not java_files:
        logger.warning(f"No Java files found in repository {repo}")
        return {
            "repo": repo,
            "instance_id": instance_id,
            "suspicious_files": []
        }

    # Initialize retriever
    retriever = FileRetriever(embedding_model)

    # Retrieve suspicious files
    suspicious_files = retriever.retrieve_suspicious_files(
        problem_statement,
        java_files,
        top_n
    )

    return {
        "repo": repo,
        "instance_id": instance_id,
        "suspicious_files": suspicious_files
    }

!pip install sentence_transformers

import os
import json
import logging
from tqdm import tqdm
from datasets import load_dataset
from google.colab import drive
import time

from sentence_transformers import SentenceTransformer

############################
# Local Embedding Model
############################
class LocalEmbeddingModel:
    """
    Example local embedding model using SentenceTransformers.
    You can choose different model names as needed.
    """
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)

    def get_text_embedding(self, text: str):
        """
        Returns a single embedding vector for the given text.
        """
        emb = self.model.encode(text, convert_to_numpy=True)
        return emb.tolist()

    def get_text_embedding_batch(self, texts: list[str]):
        """
        Returns a list of embedding vectors for the given texts.
        """
        embs = self.model.encode(texts, convert_to_numpy=True)
        return [vec.tolist() for vec in embs]


# Mount Google Drive
drive.mount('/content/drive')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Define parameters directly (instead of using argparse)
output = "/content/drive/MyDrive/suspicious_files_complex.json"  # Save to Google Drive
top_n = 50
clone_dir = "temp_repos"
dataset_name = "Daoguang/Multi-SWE-bench"
split_name = "java_verified"


def safe_process_instance(instance_data, tree, embedding_model, top_n, clone_dir, max_retries=5):
    """
    Calls process_instance, retrying up to `max_retries` times if we see a 429 RESOURCE_EXHAUSTED error.
    (In local embedding usage, you likely won't see that, but we keep the code for general error handling.)
    """
    for attempt in range(max_retries):
        try:
            return process_instance(instance_data, tree, embedding_model, top_n, clone_dir)
        except Exception as e:
            error_msg = str(e)
            # Check if it is a rate-limiting / RESOURCE_EXHAUSTED issue
            if "RESOURCE_EXHAUSTED" in error_msg or "429" in error_msg:
                logger.error(f"Hit quota limit on attempt {attempt+1}/{max_retries}. Sleeping 60 seconds and retrying...")
                time.sleep(60)
            else:
                # Not a resource-exhausted error; re-raise
                raise e

    # If we exhausted retries, raise or return an empty fallback
    logger.error("Exceeded max retries, returning empty result for this instance.")
    return {
        "repo": instance_data.get("repo"),
        "instance_id": instance_data.get("instance_id"),
        "suspicious_files": []
    }

############################
# Main Script
############################

embedding_model = LocalEmbeddingModel("all-MiniLM-L6-v2")
logger.info("Initialized local embedding model: all-MiniLM-L6-v2")

logger.info(f"Loading {dataset_name} dataset")
try:
    dataset = load_dataset(dataset_name)
    java_dataset = dataset[split_name]
    logger.info(f"Loaded {len(java_dataset)} instances from dataset")

    # Create clone directory if it doesn't exist
    os.makedirs(clone_dir, exist_ok=True)

    # Load pre-crawled repository structure info
    repo_data = json.load(open("full_repo_structure.json", "r"))

    results = []
    for i in tqdm(range(len(java_dataset))):
        instance_data = java_dataset[i]
        try:
            tree = repo_data[i]['repository_structure']

            # Here we call our safe_process_instance() wrapper:
            result = safe_process_instance(
                instance_data,
                tree,
                embedding_model,
                top_n,
                clone_dir
            )
            results.append(result)

        except Exception as e:
            logger.error(f"Error processing instance {instance_data.get('instance_id')}: {e}")
            # Add empty result for failed instances
            results.append({
                "repo": instance_data.get("repo"),
                "instance_id": instance_data.get("instance_id"),
                "suspicious_files": []
            })

    # Write results to output file on Google Drive
    logger.info(f"Writing results to {output}")
    with open(output, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2)

    logger.info(f"Done! Processed {len(results)} instances")
except Exception as e:
    logger.error(f"Failed to load dataset: {e}")

import os
import json
import logging
from tqdm import tqdm
from datasets import load_dataset
from google.colab import drive
import time

# Mount Google Drive
drive.mount('/content/drive')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Define parameters directly (instead of using argparse)
output = "/content/drive/MyDrive/suspicious_files_complex.json"  # Save to Google Drive
api_key = ""  # Replace with your API key or leave empty to use environment variable
top_n = 50
clone_dir = "temp_repos"
dataset_name = "Daoguang/Multi-SWE-bench"
split_name = "java_verified"

def safe_process_instance(instance_data, tree, embedding_model, top_n, clone_dir, max_retries=5):
    """
    Calls process_instance, retrying up to max_retries times if we see a 429 RESOURCE_EXHAUSTED error.
    """
    for attempt in range(max_retries):
        try:
            return process_instance(instance_data, tree, embedding_model, top_n, clone_dir)
        except Exception as e:
            error_msg = str(e)
            # Check if it is a rate-limiting / RESOURCE_EXHAUSTED issue
            if "RESOURCE_EXHAUSTED" in error_msg or "429" in error_msg:
                logger.error(f"Hit quota limit on attempt {attempt+1}/{max_retries}. Sleeping 60 seconds and retrying...")
                time.sleep(60)
            else:
                # Not a resource-exhausted error; re-raise
                raise e

    # If we exhausted retries, raise or return an empty fallback
    logger.error("Exceeded max retries, returning empty result for this instance.")
    return {
        "repo": instance_data.get("repo"),
        "instance_id": instance_data.get("instance_id"),
        "suspicious_files": []
    }

# Get API key from defined variable or environment variable
api_key = api_key or os.environ.get("GOOGLE_API_KEY")
if not api_key:
    logger.error("Google API key must be provided either via api_key variable or GOOGLE_API_KEY environment variable")
else:
    # Initialize embedding model
    logger.info("Initializing Gemini embedding model")
    embedding_model = GoogleGenAIEmbedding(
        api_key=api_key,
        model_name="text-embedding-004",
        embed_batch_size=150,
        task_type="retrieval_document"
    )

    # Load the dataset from Hugging Face
    logger.info(f"Loading {dataset_name} dataset")
    try:
        dataset = load_dataset(dataset_name)
        java_dataset = dataset[split_name]
        logger.info(f"Loaded {len(java_dataset)} instances from dataset")

        # Create clone directory if it doesn't exist
        os.makedirs(clone_dir, exist_ok=True)

        # Load pre-crawled repository structure info
        repo_data = json.load(open("full_repo_structure.json", "r"))

        results = []
        for i in tqdm(range(len(java_dataset))):
            instance_data = java_dataset[i]
            try:
                tree = repo_data[i]['repository_structure']

                # Here we call our safe_process_instance() wrapper:
                result = safe_process_instance(
                    instance_data,
                    tree,
                    embedding_model,
                    top_n,
                    clone_dir
                )
                results.append(result)

            except Exception as e:
                logger.error(f"Error processing instance {instance_data.get('instance_id')}: {e}")
                # Add empty result for failed instances
                results.append({
                    "repo": instance_data.get("repo"),
                    "instance_id": instance_data.get("instance_id"),
                    "suspicious_files": []
                })

        # Write results to output file on Google Drive
        logger.info(f"Writing results to {output}")
        with open(output, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2)

        logger.info(f"Done! Processed {len(results)} instances")
    except Exception as e:
        logger.error(f"Failed to load dataset: {e}")

import json
import os
from typing import List, Dict, Any, Tuple

def calculate_localization_accuracy(suspicious_files_path: str,
                                    ground_truth_path: str) -> Tuple[float, float]:
    """
    Calculate the accuracy of suspicious files predictions compared to ground truth.

    Args:
        suspicious_files_path (str): Path to the JSON file containing suspicious files predictions.
        ground_truth_path (str): Path to the JSON file containing ground truth data.

    Returns:
        Tuple[float, float]: A tuple containing (superset_accuracy, touch_accuracy)
    """
    # Load the suspicious files predictions
    with open(suspicious_files_path, 'r') as f:
        suspicious_files = json.load(f)

    # Load the ground truth data
    with open(ground_truth_path, 'r') as f:
        ground_truth = json.load(f)

    # Create a mapping from instance_id to ground truth files
    ground_truth_map = {item["instance_id"]: set(item["modified_files"]) for item in ground_truth}

    total_instances = 0
    superset_count = 0
    touch_count = 0

    # Process each suspicious files prediction
    for prediction in suspicious_files:
        instance_id = prediction["instance_id"]
        if instance_id in ground_truth_map:
            total_instances += 1
            predicted_files = set(prediction["suspicious_files"])
            ground_truth_files = ground_truth_map[instance_id]

            # Check if predicted files are a superset of ground truth files
            if ground_truth_files.issubset(predicted_files):
                superset_count += 1

            # Check if there's at least one overlap between predicted and ground truth files
            if len(ground_truth_files.intersection(predicted_files)) > 0:
                touch_count += 1

    # Calculate accuracies
    print("Total instances:", total_instances)
    print("Superset count:", superset_count)
    superset_accuracy = superset_count / total_instances if total_instances > 0 else 0.0
    touch_accuracy = touch_count / total_instances if total_instances > 0 else 0.0

    return superset_accuracy, touch_accuracy

def run_evaluation():
    # Paths to your files
    suspicious_files_path = output  # Path to your suspicious files predictions
    print(suspicious_files_path)
    ground_truth_path = "/content/ground_truth.json"          # Path to ground truth data

    if os.path.exists(suspicious_files_path) and os.path.exists(ground_truth_path):
        superset_accuracy, touch_accuracy = calculate_localization_accuracy(
            suspicious_files_path, ground_truth_path
        )

        print(f"Evaluation Results:")
        print(f"- Superset Accuracy: {superset_accuracy:.4f} ({superset_accuracy*100:.2f}%)")
        print(f"- Touch Accuracy:    {touch_accuracy:.4f} ({touch_accuracy*100:.2f}%)")
    else:
        print(f"Error: One or both of the input files do not exist.")
        print(f"  - Suspicious files path: {os.path.exists(suspicious_files_path)}")
        print(f"  - Ground truth path: {os.path.exists(ground_truth_path)}")

run_evaluation()